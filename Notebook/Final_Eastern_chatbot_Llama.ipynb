{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##ü§ñ Eastern Services RAG Chatbot - Complete System\n",
        "\n",
        "**Features:**\n",
        "- ‚úÖ Multilingual Support (English, Urdu, Roman Urdu, Punjabi)\n",
        "- ‚úÖ RAG with FAISS Vector Store\n",
        "- ‚úÖ Conversation Memory (Redis + Supabase)\n",
        "- ‚úÖ Fallback Mechanism\n",
        "- ‚úÖ Metadata Filtering\n",
        "- ‚úÖ Evaluation Metrics\n",
        "- ‚úÖ Secure .env Configuration\n",
        "\n",
        "**Database Setup:**\n",
        "- **PostgreSQL**: Supabase (User data, conversation history)\n",
        "- **Redis**: Upstash (Session cache, fast memory)\n",
        "- **FAISS**: Local vector store (Document embeddings)\n",
        "```"
      ],
      "metadata": {
        "id": "h-AljtgollbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1:**Installation and Setup**\n",
        "\n",
        "Installs all necessary dependencies (langchain, faiss-cpu, sentence-transformers, huggingface-hub) and mounts Google Drive. We rely on the core packages being installed cleanly now."
      ],
      "metadata": {
        "id": "5eZSurOFrLJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 1: Installation and Setup (NO GOOGLE DRIVE)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"INSTALLING DEPENDENCIES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Install all required packages\n",
        "!pip install -q \\\n",
        "langchain-groq \\\n",
        "langchain \\\n",
        "langchain-community \\\n",
        "langchain-core \\\n",
        "langchain-text-splitters \\\n",
        "sentence-transformers \\\n",
        "huggingface-hub \\\n",
        "python-docx \\\n",
        "python-dotenv \\\n",
        "redis \\\n",
        "psycopg2-binary \\\n",
        "sqlalchemy \\\n",
        "cohere \\\n",
        "requests \\\n",
        "PyPDF2 \\\n",
        "python-pptx \\\n",
        "openpyxl \\\n",
        "fastapi \\\n",
        "uvicorn\n",
        "\n",
        "print(\"‚úÖ All packages installed!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n‚úÖ Dependencies installed successfully.\")\n",
        "print(\"‚úÖ Supported formats: DOCX, PDF, PPTX, XLSX\")\n",
        "print(\"‚úÖ Data source: GitHub repository\")\n",
        "print(\"‚úÖ Embeddings: Supabase PGVector\")\n",
        "print(\"‚úÖ Environment: Hugging Face Spaces\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uri3HqnsoeHA",
        "outputId": "f86c7cd9-0e68-4008-d7b8-664a2377cfcd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "INSTALLING DEPENDENCIES\n",
            "================================================================================\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m354.2/354.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m‚úÖ All packages installed!\n",
            "================================================================================\n",
            "\n",
            "‚úÖ Dependencies installed successfully.\n",
            "‚úÖ Supported formats: DOCX, PDF, PPTX, XLSX\n",
            "‚úÖ Data source: GitHub repository\n",
            "‚úÖ Embeddings: Supabase PGVector\n",
            "‚úÖ Environment: Hugging Face Spaces\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 2 - Load Environment Variables"
      ],
      "metadata": {
        "id": "7C32VBh4rTTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Load Environment Variables (Works in BOTH Colab AND Hugging Face Spaces)\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LOADING ENVIRONMENT VARIABLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ============================================================================\n",
        "# STRATEGY: Try multiple sources (Colab secrets, .env file, HF Spaces secrets)\n",
        "# ============================================================================\n",
        "\n",
        "GROQ_API_KEY = None\n",
        "COHERE_API_KEY = None\n",
        "SUPABASE_URI = None\n",
        "REDIS_URI = None\n",
        "\n",
        "# Method 1: Try Google Colab Secrets (if running in Colab)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    print(\"üì± Detected: Google Colab environment\")\n",
        "    print(\"   Trying to load from Colab secrets...\")\n",
        "\n",
        "    try:\n",
        "        GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "        COHERE_API_KEY = userdata.get('COHERE_API_KEY')\n",
        "        SUPABASE_URI = userdata.get('SUPABASE_URI')\n",
        "        REDIS_URI = userdata.get('REDIS_URI')\n",
        "        print(\"   ‚úÖ Loaded from Colab secrets!\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è  Colab secrets not found: {e}\")\n",
        "        print(\"   üí° To use Colab secrets:\")\n",
        "        print(\"      Go to: üîë (key icon) ‚Üí Secrets ‚Üí Add secret\")\n",
        "        print(\"      Add: GROQ_API_KEY, COHERE_API_KEY, SUPABASE_URI, REDIS_URI\")\n",
        "except ImportError:\n",
        "    print(\"üì± Not running in Google Colab\")\n",
        "\n",
        "# Method 2: Try .env file (if not loaded from Colab secrets)\n",
        "if not all([GROQ_API_KEY, COHERE_API_KEY, SUPABASE_URI, REDIS_URI]):\n",
        "    print(\"\\nüìÑ Trying to load from .env file...\")\n",
        "\n",
        "    # Try multiple possible .env file locations\n",
        "    env_paths = [\n",
        "        '/content/.env',  # Colab root\n",
        "        '/content/drive/MyDrive/Eastern_chatbot/.env',  # Google Drive\n",
        "        '.env',  # Current directory\n",
        "        os.path.expanduser('~/.env'),  # Home directory\n",
        "    ]\n",
        "\n",
        "    env_loaded = False\n",
        "    for env_path in env_paths:\n",
        "        if os.path.exists(env_path):\n",
        "            load_dotenv(env_path)\n",
        "            print(f\"   ‚úÖ Found .env file at: {env_path}\")\n",
        "            env_loaded = True\n",
        "            break\n",
        "\n",
        "    if not env_loaded:\n",
        "        print(\"   ‚ö†Ô∏è  No .env file found\")\n",
        "        print(\"   üí° To use .env file:\")\n",
        "        print(\"      Create a .env file with:\")\n",
        "        print(\"      GROQ_API_KEY=your_key\")\n",
        "        print(\"      COHERE_API_KEY=your_key\")\n",
        "        print(\"      SUPABASE_URI=your_uri\")\n",
        "        print(\"      REDIS_URI=your_uri\")\n",
        "\n",
        "# Method 3: Try environment variables (works in HF Spaces and local)\n",
        "if not all([GROQ_API_KEY, COHERE_API_KEY, SUPABASE_URI, REDIS_URI]):\n",
        "    print(\"\\nüåê Trying to load from environment variables...\")\n",
        "    GROQ_API_KEY = os.getenv('GROQ_API_KEY') or GROQ_API_KEY\n",
        "    COHERE_API_KEY = os.getenv('COHERE_API_KEY') or COHERE_API_KEY\n",
        "    SUPABASE_URI = os.getenv('SUPABASE_URI') or SUPABASE_URI\n",
        "    REDIS_URI = os.getenv('REDIS_URI') or REDIS_URI\n",
        "\n",
        "    if all([GROQ_API_KEY, COHERE_API_KEY, SUPABASE_URI, REDIS_URI]):\n",
        "        print(\"   ‚úÖ Loaded from environment variables!\")\n",
        "        print(\"   üìç This works in Hugging Face Spaces automatically\")\n",
        "\n",
        "# Final check: Load from os.getenv if still missing (after load_dotenv)\n",
        "if not GROQ_API_KEY:\n",
        "    GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
        "if not COHERE_API_KEY:\n",
        "    COHERE_API_KEY = os.getenv('COHERE_API_KEY')\n",
        "if not SUPABASE_URI:\n",
        "    SUPABASE_URI = os.getenv('SUPABASE_URI')\n",
        "if not REDIS_URI:\n",
        "    REDIS_URI = os.getenv('REDIS_URI')\n",
        "\n",
        "# ============================================================================\n",
        "# VERIFY ALL VARIABLES ARE LOADED\n",
        "# ============================================================================\n",
        "\n",
        "required_vars = {\n",
        "    'GROQ_API_KEY': GROQ_API_KEY,\n",
        "    'COHERE_API_KEY': COHERE_API_KEY,\n",
        "    'SUPABASE_URI': SUPABASE_URI,\n",
        "    'REDIS_URI': REDIS_URI\n",
        "}\n",
        "\n",
        "missing_vars = [k for k, v in required_vars.items() if not v]\n",
        "\n",
        "if missing_vars:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"‚ùå ERROR: Missing required environment variables\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Missing: {missing_vars}\\n\")\n",
        "\n",
        "    print(\"üìã SOLUTIONS:\")\n",
        "    print(\"\\n1Ô∏è‚É£  FOR GOOGLE COLAB (Recommended for testing):\")\n",
        "    print(\"   a) Click the üîë (key icon) in the left sidebar\")\n",
        "    print(\"   b) Click 'Secrets' tab\")\n",
        "    print(\"   c) Click 'Add secret' for each:\")\n",
        "    print(\"      - GROQ_API_KEY\")\n",
        "    print(\"      - COHERE_API_KEY\")\n",
        "    print(\"      - SUPABASE_URI\")\n",
        "    print(\"      - REDIS_URI\")\n",
        "    print(\"   d) Re-run this cell\")\n",
        "\n",
        "    print(\"\\n2Ô∏è‚É£  FOR LOCAL/ENV FILE:\")\n",
        "    print(\"   Create a .env file with:\")\n",
        "    print(\"   GROQ_API_KEY=your_key_here\")\n",
        "    print(\"   COHERE_API_KEY=your_key_here\")\n",
        "    print(\"   SUPABASE_URI=your_uri_here\")\n",
        "    print(\"   REDIS_URI=your_uri_here\")\n",
        "\n",
        "    print(\"\\n3Ô∏è‚É£  FOR HUGGING FACE SPACES:\")\n",
        "    print(\"   a) Go to your Space ‚Üí Settings ‚Üí Variables and secrets\")\n",
        "    print(\"   b) Click 'New secret' for each variable\")\n",
        "    print(\"   c) Add as 'Secret' (not 'Variable')\")\n",
        "    print(\"   d) Deploy\")\n",
        "\n",
        "    raise ValueError(f\"Missing environment variables: {missing_vars}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SUCCESS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ ALL ENVIRONMENT VARIABLES LOADED SUCCESSFULLY!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"   - GROQ_API_KEY: {'*' * 20} ‚úÖ\")\n",
        "print(f\"   - COHERE_API_KEY: {'*' * 20} ‚úÖ\")\n",
        "print(f\"   - SUPABASE_URI: {SUPABASE_URI[:30]}... ‚úÖ\")\n",
        "print(f\"   - REDIS_URI: {REDIS_URI[:25]}... ‚úÖ\")\n",
        "print(\"=\"*80)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agW07lkp4IGS",
        "outputId": "24ba3161-9eaa-4841-d551-72aec1471503"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "LOADING ENVIRONMENT VARIABLES\n",
            "================================================================================\n",
            "üì± Detected: Google Colab environment\n",
            "   Trying to load from Colab secrets...\n",
            "   ‚úÖ Loaded from Colab secrets!\n",
            "\n",
            "================================================================================\n",
            "‚úÖ ALL ENVIRONMENT VARIABLES LOADED SUCCESSFULLY!\n",
            "================================================================================\n",
            "   - GROQ_API_KEY: ******************** ‚úÖ\n",
            "   - COHERE_API_KEY: ******************** ‚úÖ\n",
            "   - SUPABASE_URI: postgresql://postgres.kcyrxqcv... ‚úÖ\n",
            "   - REDIS_URI: rediss://default:ARyhAAIm... ‚úÖ\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 3 - Configuration and Paths"
      ],
      "metadata": {
        "id": "K_s49f1VrJ8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Configuration and Paths (FIXED - Uses same loading method as Cell 2)\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD ENVIRONMENT VARIABLES (Same method as Cell 2)\n",
        "# ============================================================================\n",
        "\n",
        "GROQ_API_KEY = None\n",
        "COHERE_API_KEY = None\n",
        "SUPABASE_URI = None\n",
        "REDIS_URI = None\n",
        "\n",
        "# Method 1: Try Google Colab Secrets (if running in Colab)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    try:\n",
        "        GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "        COHERE_API_KEY = userdata.get('COHERE_API_KEY')\n",
        "        SUPABASE_URI = userdata.get('SUPABASE_URI')\n",
        "        REDIS_URI = userdata.get('REDIS_URI')\n",
        "    except Exception:\n",
        "        pass\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# Method 2: Try .env file (if not loaded from Colab secrets)\n",
        "if not all([GROQ_API_KEY, COHERE_API_KEY, SUPABASE_URI, REDIS_URI]):\n",
        "    env_paths = [\n",
        "        '/content/.env',\n",
        "        '/content/drive/MyDrive/Eastern_chatbot/.env',\n",
        "        '.env',\n",
        "        os.path.expanduser('~/.env'),\n",
        "    ]\n",
        "\n",
        "    for env_path in env_paths:\n",
        "        if os.path.exists(env_path):\n",
        "            load_dotenv(env_path)\n",
        "            break\n",
        "\n",
        "# Method 3: Try environment variables (works in HF Spaces and local)\n",
        "if not GROQ_API_KEY:\n",
        "    GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
        "if not COHERE_API_KEY:\n",
        "    COHERE_API_KEY = os.getenv('COHERE_API_KEY')\n",
        "if not SUPABASE_URI:\n",
        "    SUPABASE_URI = os.getenv('SUPABASE_URI')\n",
        "if not REDIS_URI:\n",
        "    REDIS_URI = os.getenv('REDIS_URI')\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "GITHUB_REPO_URL = \"https://raw.githubusercontent.com/abdullahnaseem4802-dot/Eastern-Chatbot-Data/main/\"\n",
        "\n",
        "DATA_FILES = [\n",
        "    \"Ant_Control_Services.docx\",\n",
        "    \"Bed_Bug_Treatment_Guide.docx\",\n",
        "    \"Booking_and_Scheduling.docx\",\n",
        "    \"Chemical_Safety_Information.docx\",\n",
        "    \"Cockroach_Control_Services.docx\",\n",
        "    \"Commercial_Pest_Control.docx\",\n",
        "    \"Comprehensive_FAQ.docx\",\n",
        "    \"Contact_Support_Information.docx\",\n",
        "    \"Corporate_Packages_2025.docx\",\n",
        "    \"Customer_Testimonials.docx\",\n",
        "    \"Detailed_Pricing_2025.docx\",\n",
        "    \"Emergency_Pest_Services.docx\",\n",
        "    \"Fumigation_Services.docx\",\n",
        "    \"Mosquito_Control_Services.docx\",\n",
        "    \"Pre_Construction_Treatment.docx\",\n",
        "    \"Rodent_Control_Solutions.docx\",\n",
        "    \"Seasonal_Pest_Guide.docx\",\n",
        "    \"Service_Areas_Coverage.docx\",\n",
        "    \"Seasonal_Pest_Calendar.xlsx\",\n",
        "    \"Pest_Identification_Chart.pptx\",\n",
        "    \"Monthly_Service_Plans.xlsx\",\n",
        "    \"Integrated_Pest_Management.pdf\",\n",
        "    \"Installation_Methods_Guide.pdf\",\n",
        "    \"DIY_Prevention_Tips.pptx\",\n",
        "    \"Termite_Treatment_Complete.docx\",\n",
        "    \"Warranty_Guarantee_Policy.docx\"\n",
        "]\n",
        "\n",
        "# ============================================================================\n",
        "# VERIFY ALL KEYS ARE LOADED\n",
        "# ============================================================================\n",
        "\n",
        "required_keys = {\n",
        "    'GROQ_API_KEY': GROQ_API_KEY,\n",
        "    'COHERE_API_KEY': COHERE_API_KEY,\n",
        "    'SUPABASE_URI': SUPABASE_URI,\n",
        "    'REDIS_URI': REDIS_URI\n",
        "}\n",
        "\n",
        "missing_keys = [k for k, v in required_keys.items() if not v]\n",
        "\n",
        "if missing_keys:\n",
        "    print(\"‚ùå ERROR: Missing required environment variables!\")\n",
        "    print(f\"Missing: {missing_keys}\\n\")\n",
        "    print(\"üí° SOLUTION:\")\n",
        "    print(\"   1. Make sure Cell 2 ran successfully\")\n",
        "    print(\"   2. If using Colab, add secrets in üîë ‚Üí Secrets\")\n",
        "    print(\"   3. Re-run Cell 2, then Cell 3\")\n",
        "    raise ValueError(f\"Missing: {missing_keys}\")\n",
        "\n",
        "print(\"‚úÖ Configuration loaded!\")\n",
        "print(f\"   Files: {len(DATA_FILES)}\")\n",
        "print(f\"   Source: GitHub (Public)\")\n",
        "print(f\"   ‚úÖ All API keys loaded successfully!\")\n",
        "print(\"=\"*80)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mcbgTARrMd9",
        "outputId": "5e45fb2d-4faa-4e23-ebe7-02db63c7ea40"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Configuration loaded!\n",
            "   Files: 26\n",
            "   Source: GitHub (Public)\n",
            "   ‚úÖ All API keys loaded successfully!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 4 - Database Connections (Redis + Supabase with PGVector)"
      ],
      "metadata": {
        "id": "MZVd2AR60Ms-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Initialize Database Connections (Redis + Supabase with PGVector)\n",
        "\n",
        "import redis\n",
        "from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, text\n",
        "from sqlalchemy.orm import declarative_base, sessionmaker\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CONNECTING TO DATABASES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# --- REDIS CONNECTION ---\n",
        "REDIS_URI_CLEAN = REDIS_URI.replace('REDIS_URI=', '').strip()\n",
        "\n",
        "print(\"\\nüì° Connecting to Redis (Upstash)...\")\n",
        "try:\n",
        "    redis_client = redis.from_url(\n",
        "        REDIS_URI_CLEAN,\n",
        "        decode_responses=True,\n",
        "        socket_connect_timeout=5\n",
        "    )\n",
        "    redis_client.ping()\n",
        "    print(\"‚úÖ Redis connected successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Redis connection failed: {e}\")\n",
        "    redis_client = None\n",
        "\n",
        "# --- SUPABASE (POSTGRESQL) CONNECTION WITH PGVECTOR ---\n",
        "print(\"\\nüì° Connecting to Supabase (PostgreSQL + PGVector)...\")\n",
        "\n",
        "SessionLocal = None\n",
        "ConversationHistory = None\n",
        "engine = None\n",
        "\n",
        "try:\n",
        "    # Create engine with robust settings\n",
        "    engine = create_engine(\n",
        "        SUPABASE_URI,\n",
        "        pool_pre_ping=True,\n",
        "        pool_recycle=3600,\n",
        "        pool_size=5,\n",
        "        max_overflow=10,\n",
        "        connect_args={\n",
        "            'connect_timeout': 15,\n",
        "            'application_name': 'eastern_chatbot_production'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Test connection\n",
        "    print(\"üîç Testing database connection...\")\n",
        "    with engine.connect() as conn:\n",
        "        result = conn.execute(text(\"SELECT version();\"))\n",
        "        version = result.fetchone()[0]\n",
        "        print(f\"‚úÖ Connection successful!\")\n",
        "        print(f\"   PostgreSQL: {version[:60]}...\")\n",
        "\n",
        "        # Check if pgvector extension is enabled\n",
        "        result = conn.execute(text(\"SELECT * FROM pg_extension WHERE extname = 'vector';\"))\n",
        "        if result.fetchone():\n",
        "            print(\"‚úÖ PGVector extension is enabled!\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  PGVector extension not found - enabling it now...\")\n",
        "            conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS vector;\"))\n",
        "            conn.commit()\n",
        "            print(\"‚úÖ PGVector extension enabled!\")\n",
        "\n",
        "    # Define schema\n",
        "    Base = declarative_base()\n",
        "\n",
        "    class ConversationHistory(Base):\n",
        "        __tablename__ = 'conversation_history'\n",
        "\n",
        "        id = Column(Integer, primary_key=True, autoincrement=True)\n",
        "        session_id = Column(String(100), index=True)\n",
        "        user_question = Column(Text)\n",
        "        bot_response = Column(Text)\n",
        "        language = Column(String(50))\n",
        "        timestamp = Column(DateTime, default=datetime.utcnow)\n",
        "        meta_data = Column(Text)\n",
        "\n",
        "    # Create tables\n",
        "    print(\"üìã Creating/verifying database tables...\")\n",
        "    Base.metadata.create_all(engine)\n",
        "\n",
        "    SessionLocal = sessionmaker(bind=engine)\n",
        "\n",
        "    print(\"‚úÖ Supabase connected successfully!\")\n",
        "    print(\"‚úÖ Database tables created/verified!\")\n",
        "    print(\"‚úÖ PGVector ready for embeddings!\")\n",
        "\n",
        "except Exception as e:\n",
        "    error_msg = str(e)\n",
        "    print(f\"‚ùå Supabase connection failed!\")\n",
        "    print(f\"   Error: {error_msg[:100]}...\")\n",
        "    print(\"\\nüîß TROUBLESHOOTING:\")\n",
        "    print(\"   1. Check if Supabase project is active\")\n",
        "    print(\"   2. Verify password in .env file\")\n",
        "    print(\"   3. Ensure PGVector extension is enabled in Supabase dashboard\")\n",
        "    SessionLocal = None\n",
        "    engine = None\n",
        "\n",
        "# --- SUMMARY ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATABASE CONNECTION STATUS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"{'‚úÖ' if redis_client else '‚ùå'} Redis (Upstash): {'Connected' if redis_client else 'Failed'}\")\n",
        "print(f\"{'‚úÖ' if SessionLocal else '‚ùå'} Supabase (PostgreSQL): {'Connected' if SessionLocal else 'Failed'}\")\n",
        "print(f\"{'‚úÖ' if engine else '‚ùå'} PGVector: {'Ready' if engine else 'Not Available'}\")\n",
        "\n",
        "if redis_client and SessionLocal and engine:\n",
        "    print(\"\\nüéâ FULL CLOUD SYSTEM OPERATIONAL!\")\n",
        "    print(\"   ‚úÖ Redis: Session cache\")\n",
        "    print(\"   ‚úÖ Supabase: Permanent storage + PGVector embeddings\")\n",
        "    print(\"   ‚úÖ Data Source: GitHub\")\n",
        "elif SessionLocal and engine:\n",
        "    print(\"\\n‚ö†Ô∏è  PARTIAL SYSTEM OPERATIONAL\")\n",
        "    print(\"   ‚ùå Redis: Not available\")\n",
        "    print(\"   ‚úÖ Supabase: Connected with PGVector\")\n",
        "else:\n",
        "    print(\"\\n‚ùå SYSTEM NOT OPERATIONAL\")\n",
        "    print(\"   Fix database connections before proceeding\")\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqBxj0_-Kl9y",
        "outputId": "aeba4462-4422-436e-fd4e-90d736d8413a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CONNECTING TO DATABASES\n",
            "================================================================================\n",
            "\n",
            "üì° Connecting to Redis (Upstash)...\n",
            "‚úÖ Redis connected successfully!\n",
            "\n",
            "üì° Connecting to Supabase (PostgreSQL + PGVector)...\n",
            "üîç Testing database connection...\n",
            "‚úÖ Connection successful!\n",
            "   PostgreSQL: PostgreSQL 17.6 on aarch64-unknown-linux-gnu, compiled by gc...\n",
            "‚úÖ PGVector extension is enabled!\n",
            "üìã Creating/verifying database tables...\n",
            "‚úÖ Supabase connected successfully!\n",
            "‚úÖ Database tables created/verified!\n",
            "‚úÖ PGVector ready for embeddings!\n",
            "\n",
            "================================================================================\n",
            "DATABASE CONNECTION STATUS\n",
            "================================================================================\n",
            "‚úÖ Redis (Upstash): Connected\n",
            "‚úÖ Supabase (PostgreSQL): Connected\n",
            "‚úÖ PGVector: Ready\n",
            "\n",
            "üéâ FULL CLOUD SYSTEM OPERATIONAL!\n",
            "   ‚úÖ Redis: Session cache\n",
            "   ‚úÖ Supabase: Permanent storage + PGVector embeddings\n",
            "   ‚úÖ Data Source: GitHub\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 5 - Document Loading and Chunking"
      ],
      "metadata": {
        "id": "clP8ZDTjGA1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Document Loading from GitHub (Multi-Format Support)\n",
        "\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import json\n",
        "\n",
        "def download_file_from_github(url, filename):\n",
        "    \"\"\"Download a file from GitHub.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        return response.content\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è  Failed to download {filename}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_text_from_docx(file_content):\n",
        "    \"\"\"Extract text from .docx file.\"\"\"\n",
        "    try:\n",
        "        import docx\n",
        "        doc = docx.Document(BytesIO(file_content))\n",
        "        full_text = []\n",
        "        for para in doc.paragraphs:\n",
        "            if para.text.strip():\n",
        "                full_text.append(para.text)\n",
        "\n",
        "        # Also extract text from tables\n",
        "        for table in doc.tables:\n",
        "            for row in table.rows:\n",
        "                row_text = ' | '.join([cell.text.strip() for cell in row.cells if cell.text.strip()])\n",
        "                if row_text:\n",
        "                    full_text.append(row_text)\n",
        "\n",
        "        return '\\n\\n'.join(full_text)\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error extracting DOCX: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_text_from_pdf(file_content):\n",
        "    \"\"\"Extract text from PDF file.\"\"\"\n",
        "    try:\n",
        "        import PyPDF2\n",
        "        pdf_file = BytesIO(file_content)\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "        full_text = []\n",
        "        for page_num, page in enumerate(pdf_reader.pages):\n",
        "            text = page.extract_text()\n",
        "            if text.strip():\n",
        "                full_text.append(text)\n",
        "\n",
        "        return '\\n\\n'.join(full_text)\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error extracting PDF: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_text_from_pptx(file_content):\n",
        "    \"\"\"Extract text from PowerPoint file.\"\"\"\n",
        "    try:\n",
        "        from pptx import Presentation\n",
        "        pptx_file = BytesIO(file_content)\n",
        "        prs = Presentation(pptx_file)\n",
        "\n",
        "        full_text = []\n",
        "        for slide_num, slide in enumerate(prs.slides, 1):\n",
        "            slide_text = []\n",
        "            for shape in slide.shapes:\n",
        "                if hasattr(shape, \"text\") and shape.text.strip():\n",
        "                    slide_text.append(shape.text)\n",
        "\n",
        "            if slide_text:\n",
        "                full_text.append(f\"Slide {slide_num}:\\n\" + '\\n'.join(slide_text))\n",
        "\n",
        "        return '\\n\\n'.join(full_text)\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error extracting PPTX: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_text_from_excel(file_content):\n",
        "    \"\"\"Extract text from Excel file.\"\"\"\n",
        "    try:\n",
        "        import openpyxl\n",
        "        excel_file = BytesIO(file_content)\n",
        "        workbook = openpyxl.load_workbook(excel_file, data_only=True)\n",
        "\n",
        "        full_text = []\n",
        "        for sheet_name in workbook.sheetnames:\n",
        "            sheet = workbook[sheet_name]\n",
        "            sheet_text = [f\"Sheet: {sheet_name}\"]\n",
        "\n",
        "            for row in sheet.iter_rows(values_only=True):\n",
        "                row_text = ' | '.join([str(cell) for cell in row if cell is not None])\n",
        "                if row_text.strip():\n",
        "                    sheet_text.append(row_text)\n",
        "\n",
        "            if len(sheet_text) > 1:  # More than just the sheet name\n",
        "                full_text.append('\\n'.join(sheet_text))\n",
        "\n",
        "        return '\\n\\n'.join(full_text)\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error extracting Excel: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_text_from_file(file_content, filename):\n",
        "    \"\"\"Extract text based on file extension.\"\"\"\n",
        "    extension = filename.lower().split('.')[-1]\n",
        "\n",
        "    extractors = {\n",
        "        'docx': extract_text_from_docx,\n",
        "        'pdf': extract_text_from_pdf,\n",
        "        'pptx': extract_text_from_pptx,\n",
        "        'xlsx': extract_text_from_excel,\n",
        "        'xls': extract_text_from_excel\n",
        "    }\n",
        "\n",
        "    extractor = extractors.get(extension)\n",
        "    if extractor:\n",
        "        return extractor(file_content)\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è  Unsupported file type: .{extension}\")\n",
        "        return None\n",
        "\n",
        "def simple_chunk_text(text, chunk_size=1000, overlap=200):\n",
        "    \"\"\"Simple text chunking.\"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    text_length = len(text)\n",
        "\n",
        "    while start < text_length:\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "\n",
        "        if end < text_length:\n",
        "            for separator in ['\\n\\n', '\\n', '. ', ' ']:\n",
        "                last_sep = chunk.rfind(separator)\n",
        "                if last_sep > chunk_size * 0.5:\n",
        "                    chunk = text[start:start + last_sep + len(separator)]\n",
        "                    break\n",
        "\n",
        "        chunks.append(chunk.strip())\n",
        "        start = end - overlap\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def load_documents_from_github(repo_url, file_list):\n",
        "    \"\"\"Load all documents from GitHub repository.\"\"\"\n",
        "    all_documents = []\n",
        "    print(f\"Loading documents from GitHub repository...\")\n",
        "    print(f\"Repository: {repo_url}\")\n",
        "    print(f\"Total files to load: {len(file_list)}\\n\")\n",
        "\n",
        "    for filename in file_list:\n",
        "        file_url = repo_url + filename\n",
        "        print(f\"üì• Downloading: {filename}\")\n",
        "\n",
        "        file_content = download_file_from_github(file_url, filename)\n",
        "\n",
        "        if file_content:\n",
        "            try:\n",
        "                text = extract_text_from_file(file_content, filename)\n",
        "\n",
        "                if text:\n",
        "                    all_documents.append({\n",
        "                        'content': text,\n",
        "                        'source': filename,\n",
        "                        'file_type': filename.split('.')[-1]\n",
        "                    })\n",
        "                    print(f\"   ‚úÖ Loaded: {filename} ({len(text)} characters)\")\n",
        "                else:\n",
        "                    print(f\"   ‚ö†Ô∏è  No text extracted from {filename}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Error loading {filename}: {e}\")\n",
        "        else:\n",
        "            print(f\"   ‚ùå Skipped: {filename}\")\n",
        "\n",
        "    if not all_documents:\n",
        "        raise FileNotFoundError(f\"No documents loaded from GitHub!\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Successfully loaded {len(all_documents)} documents from GitHub\")\n",
        "    return all_documents\n",
        "\n",
        "def create_chunks_from_documents(documents):\n",
        "    \"\"\"Create chunks from all documents.\"\"\"\n",
        "    print(\"\\nSplitting documents into chunks...\")\n",
        "    all_chunks = []\n",
        "\n",
        "    for doc in documents:\n",
        "        text = doc['content']\n",
        "        source = doc['source']\n",
        "\n",
        "        text_chunks = simple_chunk_text(text, chunk_size=1000, overlap=200)\n",
        "\n",
        "        for i, chunk_text in enumerate(text_chunks):\n",
        "            chunk = {\n",
        "                'text': chunk_text,\n",
        "                'metadata': {\n",
        "                    'source': source,\n",
        "                    'file_type': doc['file_type'],\n",
        "                    'chunk_index': i,\n",
        "                    'total_chunks': len(text_chunks)\n",
        "                }\n",
        "            }\n",
        "            all_chunks.append(chunk)\n",
        "\n",
        "    print(f\"‚úÖ Created {len(all_chunks)} chunks\")\n",
        "    return all_chunks\n",
        "\n",
        "# Execute\n",
        "print(\"=\"*80)\n",
        "print(\"LOADING DATA FROM GITHUB\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "try:\n",
        "    documents = load_documents_from_github(GITHUB_REPO_URL, DATA_FILES)\n",
        "    chunks = create_chunks_from_documents(documents)\n",
        "\n",
        "    print(\"\\n‚úÖ Data preparation complete!\")\n",
        "    print(f\"   Source: GitHub Repository\")\n",
        "    print(f\"   Total Documents: {len(documents)}\")\n",
        "    print(f\"   Total Chunks: {len(chunks)}\")\n",
        "\n",
        "    LOADED_CHUNKS = chunks\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR in data loading: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeCnq3Ve2KFE",
        "outputId": "6fe3b652-db03-4749-c943-8b095f15156a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "LOADING DATA FROM GITHUB\n",
            "================================================================================\n",
            "Loading documents from GitHub repository...\n",
            "Repository: https://raw.githubusercontent.com/abdullahnaseem4802-dot/Eastern-Chatbot-Data/main/\n",
            "Total files to load: 26\n",
            "\n",
            "üì• Downloading: Ant_Control_Services.docx\n",
            "   ‚úÖ Loaded: Ant_Control_Services.docx (728 characters)\n",
            "üì• Downloading: Bed_Bug_Treatment_Guide.docx\n",
            "   ‚úÖ Loaded: Bed_Bug_Treatment_Guide.docx (1128 characters)\n",
            "üì• Downloading: Booking_and_Scheduling.docx\n",
            "   ‚úÖ Loaded: Booking_and_Scheduling.docx (963 characters)\n",
            "üì• Downloading: Chemical_Safety_Information.docx\n",
            "   ‚úÖ Loaded: Chemical_Safety_Information.docx (2849 characters)\n",
            "üì• Downloading: Cockroach_Control_Services.docx\n",
            "   ‚úÖ Loaded: Cockroach_Control_Services.docx (1575 characters)\n",
            "üì• Downloading: Commercial_Pest_Control.docx\n",
            "   ‚úÖ Loaded: Commercial_Pest_Control.docx (2678 characters)\n",
            "üì• Downloading: Comprehensive_FAQ.docx\n",
            "   ‚úÖ Loaded: Comprehensive_FAQ.docx (3542 characters)\n",
            "üì• Downloading: Contact_Support_Information.docx\n",
            "   ‚úÖ Loaded: Contact_Support_Information.docx (2067 characters)\n",
            "üì• Downloading: Corporate_Packages_2025.docx\n",
            "   ‚úÖ Loaded: Corporate_Packages_2025.docx (1196 characters)\n",
            "üì• Downloading: Customer_Testimonials.docx\n",
            "   ‚úÖ Loaded: Customer_Testimonials.docx (1207 characters)\n",
            "üì• Downloading: Detailed_Pricing_2025.docx\n",
            "   ‚úÖ Loaded: Detailed_Pricing_2025.docx (1997 characters)\n",
            "üì• Downloading: Emergency_Pest_Services.docx\n",
            "   ‚úÖ Loaded: Emergency_Pest_Services.docx (1091 characters)\n",
            "üì• Downloading: Fumigation_Services.docx\n",
            "   ‚úÖ Loaded: Fumigation_Services.docx (1103 characters)\n",
            "üì• Downloading: Mosquito_Control_Services.docx\n",
            "   ‚úÖ Loaded: Mosquito_Control_Services.docx (1915 characters)\n",
            "üì• Downloading: Pre_Construction_Treatment.docx\n",
            "   ‚úÖ Loaded: Pre_Construction_Treatment.docx (1159 characters)\n",
            "üì• Downloading: Rodent_Control_Solutions.docx\n",
            "   ‚úÖ Loaded: Rodent_Control_Solutions.docx (1805 characters)\n",
            "üì• Downloading: Seasonal_Pest_Guide.docx\n",
            "   ‚úÖ Loaded: Seasonal_Pest_Guide.docx (1000 characters)\n",
            "üì• Downloading: Service_Areas_Coverage.docx\n",
            "   ‚úÖ Loaded: Service_Areas_Coverage.docx (864 characters)\n",
            "üì• Downloading: Seasonal_Pest_Calendar.xlsx\n",
            "   ‚úÖ Loaded: Seasonal_Pest_Calendar.xlsx (1997 characters)\n",
            "üì• Downloading: Pest_Identification_Chart.pptx\n",
            "   ‚úÖ Loaded: Pest_Identification_Chart.pptx (269 characters)\n",
            "üì• Downloading: Monthly_Service_Plans.xlsx\n",
            "   ‚úÖ Loaded: Monthly_Service_Plans.xlsx (2339 characters)\n",
            "üì• Downloading: Integrated_Pest_Management.pdf\n",
            "   ‚úÖ Loaded: Integrated_Pest_Management.pdf (2452 characters)\n",
            "üì• Downloading: Installation_Methods_Guide.pdf\n",
            "   ‚úÖ Loaded: Installation_Methods_Guide.pdf (1951 characters)\n",
            "üì• Downloading: DIY_Prevention_Tips.pptx\n",
            "   ‚úÖ Loaded: DIY_Prevention_Tips.pptx (888 characters)\n",
            "üì• Downloading: Termite_Treatment_Complete.docx\n",
            "   ‚úÖ Loaded: Termite_Treatment_Complete.docx (1388 characters)\n",
            "üì• Downloading: Warranty_Guarantee_Policy.docx\n",
            "   ‚úÖ Loaded: Warranty_Guarantee_Policy.docx (1685 characters)\n",
            "\n",
            "‚úÖ Successfully loaded 26 documents from GitHub\n",
            "\n",
            "Splitting documents into chunks...\n",
            "‚úÖ Created 67 chunks\n",
            "\n",
            "‚úÖ Data preparation complete!\n",
            "   Source: GitHub Repository\n",
            "   Total Documents: 26\n",
            "   Total Chunks: 67\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 6 - Create/Load FAISS Index"
      ],
      "metadata": {
        "id": "eqZV_H1OHSwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: COMPREHENSIVE URDU TRANSLITERATION + KEYWORD RE-RANKING\n",
        "\n",
        "import cohere\n",
        "from sqlalchemy import text\n",
        "import json\n",
        "import re\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PGVECTOR EMBEDDINGS SETUP\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Initialize Cohere client\n",
        "print(\"\\nüîå Initializing Cohere embeddings...\")\n",
        "try:\n",
        "    cohere_client = cohere.Client(COHERE_API_KEY)\n",
        "    print(\"‚úÖ Cohere client initialized!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Cohere initialization failed: {e}\")\n",
        "    raise\n",
        "\n",
        "# Create embeddings table in Supabase if it doesn't exist\n",
        "print(\"\\nüìã Setting up embeddings table in Supabase...\")\n",
        "\n",
        "try:\n",
        "    with engine.connect() as conn:\n",
        "        # Create embeddings table with vector column\n",
        "        conn.execute(text(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS document_embeddings (\n",
        "                id SERIAL PRIMARY KEY,\n",
        "                chunk_text TEXT NOT NULL,\n",
        "                embedding vector(1024),\n",
        "                metadata JSONB,\n",
        "                created_at TIMESTAMP DEFAULT NOW()\n",
        "            );\n",
        "        \"\"\"))\n",
        "\n",
        "        # Create index for faster similarity search\n",
        "        conn.execute(text(\"\"\"\n",
        "            CREATE INDEX IF NOT EXISTS embedding_idx\n",
        "            ON document_embeddings\n",
        "            USING ivfflat (embedding vector_cosine_ops)\n",
        "            WITH (lists = 100);\n",
        "        \"\"\"))\n",
        "\n",
        "        conn.commit()\n",
        "        print(\"‚úÖ Embeddings table created/verified!\")\n",
        "\n",
        "        # Check if embeddings already exist\n",
        "        result = conn.execute(text(\"SELECT COUNT(*) FROM document_embeddings;\"))\n",
        "        existing_count = result.fetchone()[0]\n",
        "\n",
        "        if existing_count > 0:\n",
        "            print(f\"‚úÖ Found {existing_count} existing embeddings in database\")\n",
        "            print(\"   Skipping re-embedding (data already indexed)\")\n",
        "            embeddings_exist = True\n",
        "        else:\n",
        "            print(\"   No existing embeddings found\")\n",
        "            embeddings_exist = False\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error setting up embeddings table: {e}\")\n",
        "    raise\n",
        "\n",
        "# Generate and store embeddings if they don't exist\n",
        "if not embeddings_exist:\n",
        "    print(\"\\nüîÑ Generating embeddings for all chunks...\")\n",
        "    print(f\"   Total chunks to embed: {len(LOADED_CHUNKS)}\")\n",
        "    print(\"   This may take a few minutes...\")\n",
        "\n",
        "    try:\n",
        "        # Process in batches (Cohere API limit: 96 texts per call)\n",
        "        batch_size = 90\n",
        "        total_embedded = 0\n",
        "\n",
        "        for i in range(0, len(LOADED_CHUNKS), batch_size):\n",
        "            batch = LOADED_CHUNKS[i:i + batch_size]\n",
        "            batch_texts = [chunk['text'] for chunk in batch]\n",
        "\n",
        "            print(f\"   Processing batch {i//batch_size + 1}/{(len(LOADED_CHUNKS)-1)//batch_size + 1}...\")\n",
        "\n",
        "            # Get embeddings from Cohere\n",
        "            response = cohere_client.embed(\n",
        "                texts=batch_texts,\n",
        "                model='embed-english-v3.0',\n",
        "                input_type='search_document'\n",
        "            )\n",
        "\n",
        "            embeddings_batch = response.embeddings\n",
        "\n",
        "            # Insert into Supabase - USE RAW CONNECTION\n",
        "            raw_conn = engine.raw_connection()\n",
        "            try:\n",
        "                cursor = raw_conn.cursor()\n",
        "\n",
        "                for chunk, embedding in zip(batch, embeddings_batch):\n",
        "                    # Convert embedding to PostgreSQL vector format\n",
        "                    embedding_str = '[' + ','.join(map(str, embedding)) + ']'\n",
        "                    metadata_json = json.dumps(chunk['metadata'])\n",
        "\n",
        "                    cursor.execute(\"\"\"\n",
        "                        INSERT INTO document_embeddings (chunk_text, embedding, metadata)\n",
        "                        VALUES (%s, %s::vector, %s::jsonb)\n",
        "                    \"\"\", (chunk['text'], embedding_str, metadata_json))\n",
        "\n",
        "                    total_embedded += 1\n",
        "\n",
        "                raw_conn.commit()\n",
        "                cursor.close()\n",
        "            finally:\n",
        "                raw_conn.close()\n",
        "\n",
        "            print(f\"   ‚úÖ Embedded and stored {total_embedded}/{len(LOADED_CHUNKS)} chunks\")\n",
        "\n",
        "        print(f\"\\n‚úÖ All embeddings generated and stored in Supabase!\")\n",
        "        print(f\"   Total chunks embedded: {total_embedded}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating embeddings: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "# COMPREHENSIVE Urdu transliteration helper\n",
        "def transliterate_urdu_to_roman(text):\n",
        "    \"\"\"Comprehensive Urdu to Roman transliteration for better embedding matching.\"\"\"\n",
        "    urdu_pattern = r'[\\u0600-\\u06FF]'\n",
        "    if not re.search(urdu_pattern, text):\n",
        "        return text\n",
        "\n",
        "    # EXPANDED: 200+ word mapping\n",
        "    urdu_to_roman = {\n",
        "        # Core pest terms\n",
        "        'ÿØ€åŸÖ⁄©': 'deemak termite white ant wood damage',\n",
        "        '⁄©⁄æŸπŸÖŸÑ': 'khatmal bed bug bedbug mattress',\n",
        "        '⁄©ÿß⁄©ÿ±Ÿà⁄Ü': 'cockroach roach kitchen pest',\n",
        "        'ŸÖ⁄Ü⁄æÿ±': 'mosquito machar dengue malaria',\n",
        "        '⁄ÜŸà€Å€í': 'rat rodent mouse chooha chuhay',\n",
        "        '⁄ÜŸà€Åÿß': 'rat rodent mouse chooha',\n",
        "        '⁄Ü€åŸàŸÜŸπ€å': 'ant chiti sugar ant',\n",
        "\n",
        "        # Service terms\n",
        "        'ÿπŸÑÿßÿ¨': 'ilaj treatment control service',\n",
        "        'ÿ≥ÿ±Ÿàÿ≥': 'service treatment ilaj',\n",
        "        'ÿÆÿØŸÖÿßÿ™': 'services khidmat',\n",
        "        '⁄©ŸÜŸπÿ±ŸàŸÑ': 'control treatment',\n",
        "        'ÿ≥Ÿæÿ±€í': 'spray chemical treatment',\n",
        "        'ŸÅ€åŸàŸÖ€å⁄Ø€åÿ¥ŸÜ': 'fumigation',\n",
        "        'ŸÖÿπÿßÿ¶ŸÜ€Å': 'inspection check survey',\n",
        "\n",
        "        # Pricing terms\n",
        "        'ÿ±€åŸπ': 'rate price cost qeemat',\n",
        "        'ŸÇ€åŸÖÿ™': 'qeemat price rate cost',\n",
        "        'ÿ±ŸÇŸÖ': 'amount money price',\n",
        "        'ÿ±ŸàŸæ€í': 'rupees rs money',\n",
        "        'ŸÅ€å': 'per',\n",
        "        'ŸÖÿ±ÿ®ÿπ': 'square sq',\n",
        "        'ŸÅŸπ': 'foot ft feet',\n",
        "        'ŸÖÿ±ŸÑ€Å': 'marla',\n",
        "        '⁄©ŸÜÿßŸÑ': 'kanal',\n",
        "\n",
        "        # Guarantee terms\n",
        "        '⁄Øÿßÿ±ŸÜŸπ€å': 'guarantee warranty assurance',\n",
        "        'ÿ∂ŸÖÿßŸÜÿ™': 'guarantee warranty',\n",
        "        'Ÿàÿßÿ±ŸÜŸπ€å': 'warranty guarantee',\n",
        "\n",
        "        # Time terms\n",
        "        '⁄©ÿ™ŸÜ€å': 'kitni how much how many',\n",
        "        '⁄©ÿ™ŸÜÿß': 'kitna how much',\n",
        "        '⁄©ÿ®': 'kab when',\n",
        "        '⁄©€åÿ≥€í': 'kaisay how',\n",
        "        '⁄©€åŸà⁄∫': 'kyun why',\n",
        "        'ŸàŸÇÿ™': 'waqt time duration',\n",
        "        'ÿØŸÜ': 'day din',\n",
        "        'ŸÖÿß€Å': 'month mahina',\n",
        "        'ÿ≥ÿßŸÑ': 'year saal',\n",
        "\n",
        "        # Location terms\n",
        "        '⁄Ø⁄æÿ±': 'ghar home house residence',\n",
        "        'ŸÖ⁄©ÿßŸÜ': 'house home makan',\n",
        "        'ÿπŸÑÿßŸÇ€Å': 'area ilaqa region',\n",
        "        'ÿπŸÑÿßŸÇ€í': 'areas ilaqay regions',\n",
        "        'ÿ¥€Åÿ±': 'city shahar',\n",
        "        'ŸÑ⁄©⁄ë€å': 'wood lakri timber',\n",
        "        'ÿ™ÿπŸÖ€åÿ±': 'construction building tameer',\n",
        "\n",
        "        # Question words\n",
        "        '⁄©€åÿß': 'kya kia what',\n",
        "        '⁄©ŸàŸÜ': 'kon kaun which who what',\n",
        "        '⁄©€Åÿß⁄∫': 'kahan where',\n",
        "        '⁄©ÿ≥': 'kis which',\n",
        "        '⁄©ŸàŸÜÿ≥ÿß': 'konsa which',\n",
        "        '⁄©ŸàŸÜÿ≥€å': 'konsi which',\n",
        "        '⁄©ŸàŸÜÿ≥€í': 'konsay which',\n",
        "\n",
        "        # Common verbs\n",
        "        '€Å€í': 'hai is',\n",
        "        '€Å€å⁄∫': 'hain are',\n",
        "        '€ÅŸà': 'ho',\n",
        "        '⁄Øÿß': 'ga will',\n",
        "        '⁄Ø€å': 'gi will',\n",
        "        '⁄Ø€í': 'gay will',\n",
        "        '€ÅŸà⁄Øÿß': 'hoga will be',\n",
        "        '€ÅŸà⁄Ø€å': 'hogi will be',\n",
        "        '€ÅŸà⁄Ø€í': 'hogay will be',\n",
        "        '⁄©ÿ±€å⁄∫': 'karein do',\n",
        "        '⁄©ÿ±ŸÜÿß': 'karna to do',\n",
        "        'ÿØ€å⁄∫': 'dein give',\n",
        "        'ÿØ€åŸÜÿß': 'dena to give',\n",
        "        'ÿ¢ÿ¶€å⁄∫': 'aayein come',\n",
        "        'ÿ¢ŸÜÿß': 'aana to come',\n",
        "        'ÿ¨ÿßÿ¶€å⁄∫': 'jayein go',\n",
        "        'ÿ¨ÿßŸÜÿß': 'jana to go',\n",
        "\n",
        "        # Possessives\n",
        "        'ŸÖ€åÿ±€í': 'meray mery my',\n",
        "        'ŸÖ€åÿ±€å': 'meri my',\n",
        "        'ŸÖ€åÿ±ÿß': 'mera my',\n",
        "        'ÿ¢Ÿæ': 'aap you',\n",
        "        'ÿ¢Ÿæ⁄©ÿß': 'aapka your',\n",
        "        'ÿ¢Ÿæ⁄©€å': 'aapki your',\n",
        "        'ÿ¢Ÿæ⁄©€í': 'aapkay your',\n",
        "        '€ÅŸÖÿßÿ±€í': 'humare our',\n",
        "        '€ÅŸÖÿßÿ±€å': 'humari our',\n",
        "        '€ÅŸÖÿßÿ±ÿß': 'humara our',\n",
        "\n",
        "        # Prepositions\n",
        "        'ŸÖ€å⁄∫': 'mein in',\n",
        "        '⁄©€í': 'ke kay of',\n",
        "        '⁄©€å': 'ki of',\n",
        "        '⁄©ÿß': 'ka of',\n",
        "        'ÿ≥€í': 'se say from with',\n",
        "        '⁄©Ÿà': 'ko to',\n",
        "        'ŸÜ€í': 'ne',\n",
        "        'Ÿæÿ±': 'par on',\n",
        "        'ŸÑ€å€í': 'liye liyay for',\n",
        "        'ÿ®ÿßÿ±€í': 'baray about',\n",
        "        'ÿ≥ÿßÿ™⁄æ': 'saath with',\n",
        "\n",
        "        # Adjectives\n",
        "        'ÿ®€Åÿ™': 'bohat bahut very much',\n",
        "        'ÿ≤€åÿßÿØ€Å': 'zyada more',\n",
        "        '⁄©ŸÖ': 'kam less',\n",
        "        'ÿ®⁄ëÿß': 'bara big large',\n",
        "        '⁄Ü⁄æŸàŸπÿß': 'chota small',\n",
        "        'ÿß⁄Ü⁄æÿß': 'acha good',\n",
        "        'ÿ®€Åÿ™ÿ±€åŸÜ': 'behtareen best excellent',\n",
        "        'Ÿæ€åÿ¥€Å Ÿàÿ±ÿßŸÜ€Å': 'professional peshawrana',\n",
        "        'ŸÖÿπ€åÿßÿ±€å': 'quality standard',\n",
        "\n",
        "        # Common nouns\n",
        "        'ŸÖÿ≥ÿ¶ŸÑ€Å': 'masla problem issue',\n",
        "        'ÿ≠ŸÑ': 'solution hal',\n",
        "        'ÿ∑ÿ±€åŸÇ€Å': 'method tariqa',\n",
        "        'Ÿæ€å⁄©ÿ¨': 'package',\n",
        "        'ÿ®⁄©ŸÜ⁄Ø': 'booking',\n",
        "        'ÿ¥€å⁄àŸàŸÑŸÜ⁄Ø': 'scheduling',\n",
        "        'ÿ±ÿßÿ®ÿ∑€Å': 'contact rabta',\n",
        "        'ŸÖÿØÿØ': 'help madad',\n",
        "        'ŸÖÿπŸÑŸàŸÖÿßÿ™': 'information maloomat',\n",
        "        'ÿ™ŸÅÿµ€åŸÑ': 'details tafseel',\n",
        "\n",
        "        # Negation\n",
        "        'ŸÜ€Å€å⁄∫': 'nahi no not',\n",
        "        'ŸÜ€Å': 'na no not',\n",
        "\n",
        "        # Conjunctions\n",
        "        'ÿßŸàÿ±': 'aur and',\n",
        "        '€åÿß': 'ya or',\n",
        "        'ŸÑ€å⁄©ŸÜ': 'lekin but',\n",
        "        'ŸÖ⁄Øÿ±': 'magar but',\n",
        "        'ÿ™Ÿà': 'to then',\n",
        "        'ÿß⁄Øÿ±': 'agar if',\n",
        "\n",
        "        # Polite terms\n",
        "        'ÿ®ÿ±ÿß€Å': 'barah please',\n",
        "        '⁄©ÿ±ŸÖ': 'karam kindness',\n",
        "        'ÿ¥⁄©ÿ±€å€Å': 'shukriya thanks',\n",
        "        'ŸÖÿπÿ∞ÿ±ÿ™': 'maazrat sorry',\n",
        "        'ÿÆŸàÿ¥': 'khush happy',\n",
        "        'ÿ¢ŸÖÿØ€åÿØ': 'aamdeed welcome',\n",
        "\n",
        "        # Business terms\n",
        "        '⁄©ŸÖŸæŸÜ€å': 'company',\n",
        "        'ŸÅÿ±ŸÖ': 'firm',\n",
        "        'Ÿπ€åŸÖ': 'team',\n",
        "        'ŸÖÿß€Åÿ±': 'expert mahir',\n",
        "        'ÿ™ÿ¨ÿ±ÿ®€Å': 'experience tajurba',\n",
        "        'ÿ∂ÿ±Ÿàÿ±ÿ™': 'need zaroorat',\n",
        "        'ŸÅŸàÿ±€å': 'urgent fori',\n",
        "        'ÿß€åŸÖÿ±ÿ¨ŸÜÿ≥€å': 'emergency',\n",
        "\n",
        "        # Additional context\n",
        "        'Ÿæ€ÅŸÑ€í': 'pehlay before first',\n",
        "        'ÿ®ÿπÿØ': 'baad after',\n",
        "        'ÿØŸàÿ±ÿßŸÜ': 'during doran',\n",
        "        'ÿÆÿ™ŸÖ': 'khatam end finish',\n",
        "        'ÿ¥ÿ±Ÿàÿπ': 'start shuru',\n",
        "        'ŸÖ⁄©ŸÖŸÑ': 'complete mukammal',\n",
        "    }\n",
        "\n",
        "    # Replace Urdu words with Roman equivalents\n",
        "    roman_text = text\n",
        "    for urdu_word, roman_word in urdu_to_roman.items():\n",
        "        roman_text = roman_text.replace(urdu_word, roman_word)\n",
        "\n",
        "    # Remove any remaining non-Roman/non-space characters\n",
        "    roman_text = re.sub(r'[^\\w\\s]', ' ', roman_text)\n",
        "    roman_text = re.sub(r'\\s+', ' ', roman_text)\n",
        "\n",
        "    return roman_text.strip()\n",
        "\n",
        "# Create retriever function for PGVector with KEYWORD RE-RANKING\n",
        "def retrieve_similar_chunks(query, k=3):\n",
        "    \"\"\"Retrieve similar chunks from PGVector with Urdu support and keyword re-ranking.\"\"\"\n",
        "    try:\n",
        "        # Transliterate Urdu to Roman for better matching\n",
        "        search_query = transliterate_urdu_to_roman(query)\n",
        "        if search_query != query:\n",
        "            print(f\"   üîÑ Transliterated: '{query[:40]}...' ‚Üí '{search_query[:40]}...'\")\n",
        "\n",
        "        # Generate query embedding\n",
        "        query_response = cohere_client.embed(\n",
        "            texts=[search_query],\n",
        "            model='embed-english-v3.0',\n",
        "            input_type='search_query'\n",
        "        )\n",
        "        query_embedding = query_response.embeddings[0]\n",
        "        query_embedding_str = '[' + ','.join(map(str, query_embedding)) + ']'\n",
        "\n",
        "        # RETRIEVE MORE chunks for re-ranking (5 instead of 3)\n",
        "        retrieval_k = k + 2\n",
        "\n",
        "        # Search for similar chunks\n",
        "        raw_conn = engine.raw_connection()\n",
        "        try:\n",
        "            cursor = raw_conn.cursor()\n",
        "\n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT chunk_text, metadata,\n",
        "                       1 - (embedding <=> %s::vector) as similarity\n",
        "                FROM document_embeddings\n",
        "                ORDER BY embedding <=> %s::vector\n",
        "                LIMIT %s\n",
        "            \"\"\", (query_embedding_str, query_embedding_str, retrieval_k))\n",
        "\n",
        "            rows = cursor.fetchall()\n",
        "            cursor.close()\n",
        "\n",
        "            documents = []\n",
        "            for row in rows:\n",
        "                metadata = row[1] if isinstance(row[1], dict) else (json.loads(row[1]) if row[1] else {})\n",
        "                doc = {\n",
        "                    'page_content': row[0],\n",
        "                    'metadata': metadata,\n",
        "                    'similarity': row[2]\n",
        "                }\n",
        "                documents.append(doc)\n",
        "\n",
        "            # KEYWORD-BASED RE-RANKING\n",
        "            query_lower = search_query.lower()\n",
        "\n",
        "            # Define pest-specific keywords\n",
        "            pest_keywords = {\n",
        "                'deemak': ['termite', 'deemak', 'white ant', 'wood', 'termite treatment', 'post-construction', 'pre-construction'],\n",
        "                'termite': ['termite', 'deemak', 'white ant', 'wood', 'termite treatment', 'post-construction', 'pre-construction'],\n",
        "                'khatmal': ['bed bug', 'khatmal', 'bedbug', 'mattress', 'bed bug treatment'],\n",
        "                'bed': ['bed bug', 'khatmal', 'bedbug', 'mattress', 'bed bug treatment'],\n",
        "                'bug': ['bed bug', 'khatmal', 'bedbug', 'mattress', 'bed bug treatment'],\n",
        "                'cockroach': ['cockroach', 'roach', 'kitchen', 'cockroach control'],\n",
        "                'mosquito': ['mosquito', 'malaria', 'dengue', 'mosquito control', 'machar'],\n",
        "                'rat': ['rat', 'rodent', 'mouse', 'chuhay', 'chooha', 'rodent control'],\n",
        "                'chuhay': ['rat', 'rodent', 'mouse', 'chuhay', 'chooha', 'rodent control'],\n",
        "                'ant': ['ant', 'chiti', 'sugar ant', 'ant control'],\n",
        "                'guarantee': ['guarantee', 'warranty', 'guaranteed', 'assurance'],\n",
        "                'rate': ['rate', 'price', 'cost', 'pricing', 'charges', 'rupees', 'rs', 'qeemat', 'per sq ft'],\n",
        "                'service': ['service', 'services', 'provide', 'offer', 'ilaj', 'treatment', 'control'],\n",
        "                'marla': ['marla', 'sq ft', 'square', 'area', 'size', 'construction'],\n",
        "                'kanal': ['kanal', 'sq ft', 'square', 'area', 'size', 'construction'],\n",
        "            }\n",
        "\n",
        "            # Find which pest/topic is mentioned in query\n",
        "            query_relevant_keywords = []\n",
        "            for key, keywords in pest_keywords.items():\n",
        "                if any(kw in query_lower for kw in [key]):\n",
        "                    query_relevant_keywords.extend(keywords)\n",
        "\n",
        "            # Ensure unique keywords\n",
        "            query_relevant_keywords = list(set(query_relevant_keywords))\n",
        "\n",
        "            # Re-rank documents based on keyword matches\n",
        "            if query_relevant_keywords:\n",
        "                for doc in documents:\n",
        "                    doc_text_lower = doc['page_content'].lower()\n",
        "                    keyword_matches = sum(1 for kw in query_relevant_keywords if kw in doc_text_lower)\n",
        "\n",
        "                    # STRONG BOOST: Each keyword match adds 0.2\n",
        "                    if keyword_matches > 0:\n",
        "                        boost = min(0.5, keyword_matches * 0.2)\n",
        "                        doc['similarity'] = min(1.0, doc['similarity'] + boost)\n",
        "                        doc['keyword_matches'] = keyword_matches\n",
        "                    else:\n",
        "                        # PENALTY: Reduce similarity by 20% if NO keywords found\n",
        "                        doc['similarity'] = doc['similarity'] * 0.8\n",
        "                        doc['keyword_matches'] = 0\n",
        "\n",
        "                # Re-sort by updated similarity\n",
        "                documents.sort(key=lambda x: x['similarity'], reverse=True)\n",
        "\n",
        "            # Return top k documents after re-ranking\n",
        "            return documents[:k]\n",
        "\n",
        "        finally:\n",
        "            raw_conn.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error retrieving similar chunks: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return []\n",
        "\n",
        "# Force functions into global scope\n",
        "globals()['transliterate_urdu_to_roman'] = transliterate_urdu_to_roman\n",
        "globals()['retrieve_similar_chunks'] = retrieve_similar_chunks\n",
        "\n",
        "print(\"\\n‚úÖ PGVector retriever function created!\")\n",
        "print(\"   Use: retrieve_similar_chunks(query, k=3)\")\n",
        "print(\"   ‚≠ê NOW WITH IMPROVED URDU TRANSLITERATION!\")\n",
        "print(\"=\"*80)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGRg4r2aMMMr",
        "outputId": "fab5d15a-fde4-4ccd-b980-ffc149b95fd2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "PGVECTOR EMBEDDINGS SETUP\n",
            "================================================================================\n",
            "\n",
            "üîå Initializing Cohere embeddings...\n",
            "‚úÖ Cohere client initialized!\n",
            "\n",
            "üìã Setting up embeddings table in Supabase...\n",
            "‚úÖ Embeddings table created/verified!\n",
            "‚úÖ Found 67 existing embeddings in database\n",
            "   Skipping re-embedding (data already indexed)\n",
            "\n",
            "‚úÖ PGVector retriever function created!\n",
            "   Use: retrieve_similar_chunks(query, k=3)\n",
            "   ‚≠ê NOW WITH IMPROVED URDU TRANSLITERATION!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 7 - Initialize LLM"
      ],
      "metadata": {
        "id": "z4uvRqEZHmbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Cell 7\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(\n",
        "    api_key=GROQ_API_KEY,\n",
        "    model=\"llama-3.3-70b-versatile\",  # ‚Üê NEWEST MODEL!\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Groq LLM initialized: Llama 3.3 70B Versatile (Latest)\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "12cR9Ld0kDXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef75f84b-a7cf-4dd9-a5d0-8666928df2df"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Groq LLM initialized: Llama 3.3 70B Versatile (Latest)\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 8 - Language Detection and Helper Functions"
      ],
      "metadata": {
        "id": "3FgAcNxAHv3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Language Detection (3 LANGUAGES ONLY - PUNJABI MAPPED TO URDU)\n",
        "\n",
        "import re\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "def detect_greeting(text):\n",
        "    \"\"\"Detect if text contains a greeting.\"\"\"\n",
        "    text_lower = text.lower()\n",
        "    urdu_pattern = r'[\\u0600-\\u06FF]'\n",
        "    has_urdu_script = re.search(urdu_pattern, text)\n",
        "\n",
        "    if 'ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ' in text or 'ÿßŸÑÿ≥ŸÑÿßŸÖ' in text:\n",
        "        return 'urdu_greeting'\n",
        "\n",
        "    if 'assalam' in text_lower or 'assalamu' in text_lower:\n",
        "        if has_urdu_script:\n",
        "            return 'urdu_greeting'\n",
        "        return 'roman_urdu_greeting'\n",
        "\n",
        "    # Punjabi greeting mapped to Roman Urdu greeting\n",
        "    if 'assalam' in text_lower or ('ki hall' in text_lower and 'ah pai' in text_lower):\n",
        "        return 'roman_urdu_greeting'  # ‚Üê MAPPED TO ROMAN URDU\n",
        "\n",
        "    if any(greet in text_lower for greet in ['hello', 'hi', 'hey', 'greetings']):\n",
        "        return 'english_greeting'\n",
        "\n",
        "    return None\n",
        "\n",
        "def detect_language(text):\n",
        "    \"\"\"\n",
        "    Detect language and MAP PUNJABI TO URDU.\n",
        "\n",
        "    Returns ONLY 3 languages:\n",
        "    - 'english' ‚Üí English response\n",
        "    - 'urdu' ‚Üí Pure Urdu response\n",
        "    - 'roman_urdu' ‚Üí Roman Urdu response\n",
        "\n",
        "    Punjabi queries are automatically mapped to appropriate Urdu.\n",
        "    \"\"\"\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Check for Nastaliq/Urdu script (includes Punjabi Nastaliq)\n",
        "    urdu_pattern = r'[\\u0600-\\u06FF]'\n",
        "    if re.search(urdu_pattern, text):\n",
        "        return 'urdu'  # ‚Üê ALL Nastaliq script = Pure Urdu response\n",
        "\n",
        "    # Punjabi keywords (will be MAPPED to Roman Urdu)\n",
        "    punjabi_keywords = [\n",
        "        'tuhada', 'tussin', 'tusi', 'ah pai', 'ki hall', 'lag gi',\n",
        "        'kaar cha', 'vich', 'da', 'di', 'oday', 'tay', 'karwana',\n",
        "        'chana', 'karday', 'keri', 'oda', 'nu', 'te', 'ae',\n",
        "        'samajh layya', 'dasso', 'layi', 'nal', 'baray', 'chahoge',\n",
        "        'meray kaar', 'wood da kam', 'treatement da', 'ki ah'\n",
        "    ]\n",
        "    punjabi_count = sum(1 for keyword in punjabi_keywords if keyword in text_lower)\n",
        "\n",
        "    # Roman Urdu keywords\n",
        "    urdu_keywords = [\n",
        "        'aapka', 'aapko', 'samajh gaya', 'hain', 'kia', 'hai',\n",
        "        'karna', 'hoga', 'meray', 'ghar', 'deemak', 'rate',\n",
        "        'ho ga', 'mein', 'ke', 'ka', 'ki', 'kitni', 'kitna',\n",
        "        'chahta', 'chahiye', 'zaroorat', 'problem', 'masla',\n",
        "        'ilaj', 'khatam', 'control', 'service', 'guarantee'\n",
        "    ]\n",
        "    urdu_count = sum(1 for keyword in urdu_keywords if keyword in text_lower)\n",
        "\n",
        "    # English keywords\n",
        "    english_keywords = [\n",
        "        'what', 'which', 'how', 'when', 'where', 'why',\n",
        "        'is', 'are', 'was', 'were', 'has', 'have', 'had',\n",
        "        'the', 'this', 'that', 'these', 'those',\n",
        "        'treatment', 'process', 'guarantee', 'period',\n",
        "        'clients', 'worked', 'with', 'services', 'offer'\n",
        "    ]\n",
        "    english_count = sum(1 for keyword in english_keywords if keyword in text_lower)\n",
        "\n",
        "    # üéØ KEY CHANGE: If Punjabi detected, MAP TO ROMAN URDU\n",
        "    if punjabi_count >= 2:\n",
        "        print(\"  [üîÑ Punjabi detected ‚Üí Mapped to Roman Urdu]\")\n",
        "        return 'roman_urdu'  # ‚Üê PUNJABI ‚Üí ROMAN URDU\n",
        "\n",
        "    # If Roman Urdu keywords are dominant\n",
        "    if urdu_count >= 3 and urdu_count > english_count:\n",
        "        return 'roman_urdu'\n",
        "\n",
        "    # If English keywords are dominant\n",
        "    if english_count >= 3:\n",
        "        return 'english'\n",
        "\n",
        "    # Fallback checks\n",
        "    if any(word in text_lower for word in ['kitni', 'kitna', 'kia', 'kya']) and english_count < 2:\n",
        "        return 'roman_urdu'\n",
        "\n",
        "    # üéØ If Punjabi indicators found, map to Roman Urdu\n",
        "    if any(word in text_lower for word in ['tusi', 'tuhada', 'karday', 'keri', 'vich', 'da di']):\n",
        "        print(\"  [üîÑ Punjabi detected ‚Üí Mapped to Roman Urdu]\")\n",
        "        return 'roman_urdu'  # ‚Üê PUNJABI ‚Üí ROMAN URDU\n",
        "\n",
        "    # Default to English if unsure\n",
        "    return 'english'\n",
        "\n",
        "def clean_response(response, language, original_question=None):\n",
        "    \"\"\"Remove unwanted meta-commentary and clean up response.\"\"\"\n",
        "    patterns = [\n",
        "        r'Q:\\s*.*?\\n\\s*A:\\s*',\n",
        "        r'Based on the provided context.*?:\\s*',\n",
        "        r'According to the context.*?:\\s*',\n",
        "        r'RULE \\d+.*?:\\s*',\n",
        "        r'INSTRUCTIONS?:.*?:\\s*',\n",
        "        r'Since the question.*?:\\s*',\n",
        "        r'The question is in.*?:\\s*',\n",
        "        r'LANGUAGE DETECTION.*?:\\s*',\n",
        "        r'I will.*?:\\s*',\n",
        "        r'Let me.*?:\\s*',\n",
        "        r'\\([^)]*\\)',\n",
        "        r'Assalam-o-Alaikum!?\\s*',\n",
        "        r'ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ\\s*',\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        response = re.sub(pattern, '', response, flags=re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "    # Remove repeated question if present\n",
        "    if original_question:\n",
        "        question_clean = original_question.strip()\n",
        "        question_clean = re.sub(r'Assalam-o-Alaikum!?\\s*', '', question_clean, flags=re.IGNORECASE)\n",
        "        question_clean = re.sub(r'ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ\\s*', '', question_clean)\n",
        "        question_clean = re.sub(r'Assalam-o-Alaikum!?\\s*', '', question_clean, flags=re.IGNORECASE)\n",
        "        question_clean = question_clean.strip()\n",
        "\n",
        "        response_stripped = response.strip()\n",
        "\n",
        "        if question_clean and response_stripped.startswith(question_clean):\n",
        "            temp_response = response_stripped[len(question_clean):].strip()\n",
        "            if len(temp_response) > 20:\n",
        "                response = temp_response\n",
        "\n",
        "    response = response.strip()\n",
        "    response = re.sub(r'\\n\\n+', '\\n', response)\n",
        "    response = re.sub(r'\\s+', ' ', response)\n",
        "\n",
        "    return response\n",
        "\n",
        "print(\"‚úÖ Language detection loaded!\")\n",
        "print(\"=\"*80)\n",
        "print(\"SUPPORTED LANGUAGES (3 ONLY):\")\n",
        "print(\"  ‚úÖ English ‚Üí English response\")\n",
        "print(\"  ‚úÖ Pure Urdu (Nastaliq) ‚Üí Pure Urdu response\")\n",
        "print(\"  ‚úÖ Roman Urdu ‚Üí Roman Urdu response\")\n",
        "print(\"=\"*80)\n",
        "print(\"LANGUAGE MAPPING:\")\n",
        "print(\"  üîÑ Punjabi (Roman) ‚Üí Roman Urdu response\")\n",
        "print(\"  üîÑ Punjabi (Nastaliq) ‚Üí Pure Urdu response\")\n",
        "print(\"=\"*80)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T6KsDFfHxYa",
        "outputId": "0444686f-3f7a-4204-e9bd-1b8b28b8a604"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Language detection loaded!\n",
            "================================================================================\n",
            "SUPPORTED LANGUAGES (3 ONLY):\n",
            "  ‚úÖ English ‚Üí English response\n",
            "  ‚úÖ Pure Urdu (Nastaliq) ‚Üí Pure Urdu response\n",
            "  ‚úÖ Roman Urdu ‚Üí Roman Urdu response\n",
            "================================================================================\n",
            "LANGUAGE MAPPING:\n",
            "  üîÑ Punjabi (Roman) ‚Üí Roman Urdu response\n",
            "  üîÑ Punjabi (Nastaliq) ‚Üí Pure Urdu response\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 9 - Language-Specific Prompts"
      ],
      "metadata": {
        "id": "83oNZdS1IAb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: ULTIMATE FIX - Language-Specific Prompts (STOP RATE OBSESSION)\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# ============================================================================\n",
        "# üö® ULTRA-NUCLEAR PROMPTS - ANSWER ONLY WHAT IS ASKED\n",
        "# ============================================================================\n",
        "\n",
        "ENGLISH_PROMPT = \"\"\"You are Eastern Services pest control bot.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "‚ö†Ô∏è CRITICAL RULES - VIOLATION = IMMEDIATE FAIL:\n",
        "1. Answer ONLY in English\n",
        "2. Use ONLY Context information - NEVER make up information\n",
        "3. Answer ONLY what the user asked - DO NOT add extra information\n",
        "4. Keep answers 2-4 sentences\n",
        "5. NO greetings unless user greets first\n",
        "6. **CONTACT INFO:** Phone: +92 336 1101234, Email: easternservices.pk@gmail.com (ALWAYS use these, IGNORE any other phone numbers in Context!)\n",
        "7. **GUARANTEE PERIODS:** Termite Treatment = 5 years, Termite Proofing = 10 years, General Pest Control = 1 month, Bed Bug Treatment = 3 months\n",
        "\n",
        "**üö® RATE/COST RULES - FOLLOW EXACTLY:**\n",
        "- ‚ùå DO NOT mention rates/prices UNLESS the question explicitly asks about: rate, cost, price, charges, kitna, kitni\n",
        "- ‚ùå DO NOT ask for area UNLESS user already mentioned a pest problem AND asked for cost calculation\n",
        "- ‚úÖ If asked \"What services?\", list services ONLY - NO rates\n",
        "- ‚úÖ If asked about process/method, explain process ONLY - NO rates\n",
        "- ‚úÖ If asked about guarantee, give guarantee period ONLY - NO rates\n",
        "\n",
        "**CALCULATION RULES (only if user asks about cost/rate):**\n",
        "- If user provides area (Marla, Kanal, sq ft), calculate total cost\n",
        "- 1 Marla = 225 sq ft, 1 Kanal = 20 Marla = 4,500 sq ft\n",
        "- Formula: Area (sq ft) √ó Rate per sq ft = Total Cost\n",
        "- Example: 10 Marla = 2,250 sq ft √ó Rs. 80/sq ft = Rs. 180,000\n",
        "\n",
        "**If user does NOT provide area and asks about rate:**\n",
        "‚úÖ CORRECT: \"Termite treatment rate is Rs. 80 per sq ft.\"\n",
        "‚ùå WRONG: \"Please tell me your area so I can calculate...\"\n",
        "\n",
        "**EXAMPLES OF CORRECT RESPONSES:**\n",
        "Q: \"What services do you offer?\"\n",
        "A: \"We offer Termite Treatment, Termite Proofing, General Pest Control, Bed Bug Treatment, Fumigation, Rodent Control, and Cockroach Control.\"\n",
        "\n",
        "Q: \"What is the termite treatment process?\"\n",
        "A: \"We inspect the property, drill holes around the foundation, inject termiticide into the soil, treat all wooden structures, and provide a 5-year guarantee.\"\n",
        "\n",
        "Q: \"Do I need to vacate during treatment?\"\n",
        "A: \"For fumigation, yes - you must vacate for 24-48 hours. For termite treatment, you can stay but avoid treated areas for 4-6 hours.\"\n",
        "\n",
        "Answer (DIRECT, NO EXTRA INFO):\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "\n",
        "ROMAN_URDU_PROMPT = \"\"\"Aap Eastern Services pest control bot hain.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "‚ö†Ô∏è ZAROORI RULES - VIOLATION = FAIL:\n",
        "1. Sirf Roman Urdu mein jawab do\n",
        "2. Sirf Context ki info use karo - KABHI sawal wapas NA poocho\n",
        "3. Sirf wahi jawab do jo user ne poocha - EXTRA info NA do\n",
        "4. Jawab 2-4 sentences mein\n",
        "5. NO salam jab tak user salam na karay\n",
        "6. **CONTACT INFO:** Phone: +92 336 1101234, Email: easternservices.pk@gmail.com (HAMESHA yeh use karo!)\n",
        "7. **GUARANTEE PERIODS:** Termite Treatment = 5 saal, Termite Proofing = 10 saal, General Pest Control = 1 mahina, Bed Bug Treatment = 3 mahine\n",
        "\n",
        "**üö® RATE/COST RULES - BILKUL FOLLOW KARO:**\n",
        "- ‚ùå Rate/price KABHI NA batao JAB TAK sawal mein yeh na ho: rate, cost, price, charges, kitna, kitni\n",
        "- ‚ùå Area KABHI NA poocho JAB TAK user ne pest problem + cost calculation dono NA bataya ho\n",
        "- ‚úÖ Agar \"Kya services hain?\" pucha, to SIRF services batao - NO rates\n",
        "- ‚úÖ Agar process/method pucha, to SIRF process batao - NO rates\n",
        "- ‚úÖ Agar guarantee pucha, to SIRF guarantee period batao - NO rates\n",
        "\n",
        "**CALCULATION RULES (sirf agar user cost/rate puchay):**\n",
        "- Agar user area bataye (Marla, Kanal, sq ft), to total cost CALCULATE karo\n",
        "- 1 Marla = 225 sq ft, 1 Kanal = 20 Marla = 4,500 sq ft\n",
        "- Formula: Area (sq ft) √ó Rate per sq ft = Total Cost\n",
        "- Example: 10 Marla = 2,250 sq ft √ó Rs. 80/sq ft = Rs. 180,000\n",
        "\n",
        "**Agar user area NAHI bataye aur rate puchay:**\n",
        "‚úÖ SAHI: \"Deemak ka rate Rs. 80 per sq ft hai.\"\n",
        "‚ùå GALAT: \"Apna area bataen to main total cost bata sakta hoon.\"\n",
        "‚ùå GALAT: \"Lekin per sq ft rate yeh hai.\"\n",
        "\n",
        "**üö® ABSOLUTE BAN - KABHI YEH NA KARO:**\n",
        "- ‚ùå NEVER say \"Agar aap...\"\n",
        "- ‚ùå NEVER say \"Apna area bataen...\"\n",
        "- ‚ùå NEVER say \"Main aapko bata sakta hoon agar...\"\n",
        "- ‚ùå NEVER ask questions back\n",
        "\n",
        "**SAHI JAWAB KI EXAMPLES:**\n",
        "Q: \"Tusi or keri keri service provide karday o?\"\n",
        "A: \"Hum yeh services provide karte hain: Termite Treatment, Termite Proofing, General Pest Control, Bed Bug Treatment, Fumigation, Rodent Control, aur Cockroach Control.\"\n",
        "\n",
        "Q: \"Deemak ka ilaj kaise hota hai?\"\n",
        "A: \"Hum property inspect karte hain, foundation ke around holes drill karte hain, soil mein termiticide inject karte hain, aur tamam wooden structures ka treatment karte hain. 5 saal ki guarantee hai.\"\n",
        "\n",
        "Q: \"Kya treatment ke dauran ghar khali karna hoga?\"\n",
        "A: \"Fumigation ke liye haan - 24-48 ghante ke liye khali karna hoga. Termite treatment ke liye aap reh sakte hain lekin treated areas se 4-6 ghante door rahein.\"\n",
        "\n",
        "**GENDER RULES (MASCULINE/FEMININE):**\n",
        "- Use MASCULINE for: deemak, khatmal, service, treatment, guarantee\n",
        "  ‚úÖ \"Deemak KA rate\" (NOT \"ki rate\")\n",
        "  ‚úÖ \"Service KI guarantee\" (NOT \"ka guarantee\")\n",
        "  ‚úÖ \"Treatment KI cost\" (NOT \"ka cost\")\n",
        "\n",
        "Jawab (SEEDHA, NO EXTRA INFO):\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "\n",
        "URDU_PROMPT = \"\"\"ÿ¢Ÿæ Eastern Services pest control bot €Å€å⁄∫€î\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "‚ö†Ô∏è ÿ∂ÿ±Ÿàÿ±€å ÿßÿµŸàŸÑ - ÿÆŸÑÿßŸÅ Ÿàÿ±ÿ≤€å = ŸÜÿß⁄©ÿßŸÖ€å:\n",
        "1. ÿµÿ±ŸÅ ÿßÿ±ÿØŸà ŸÖ€å⁄∫ ÿ¨Ÿàÿßÿ® ÿØ€å⁄∫\n",
        "2. ÿµÿ±ŸÅ Context ⁄©€å ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßÿ≥ÿ™ÿπŸÖÿßŸÑ ⁄©ÿ±€å⁄∫ - ⁄©ÿ®⁄æ€å ÿ≥ŸàÿßŸÑ ŸàÿßŸæÿ≥ ŸÜ€Å ŸæŸà⁄Ü⁄æ€å⁄∫\n",
        "3. ÿµÿ±ŸÅ Ÿà€Å€å ÿ¨Ÿàÿßÿ® ÿØ€å⁄∫ ÿ¨Ÿà ÿµÿßÿ±ŸÅ ŸÜ€í ŸæŸà⁄Ü⁄æÿß - ÿßÿ∂ÿßŸÅ€å ŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÜ€Å ÿØ€å⁄∫\n",
        "4. ÿ¨Ÿàÿßÿ® 2-4 ÿ¨ŸÖŸÑŸà⁄∫ ŸÖ€å⁄∫\n",
        "5. ÿ≥ŸÑÿßŸÖ ŸÜ€Å€å⁄∫ ÿ¨ÿ® ÿ™⁄© ÿµÿßÿ±ŸÅ ÿ≥ŸÑÿßŸÖ ŸÜ€Å ⁄©ÿ±€í\n",
        "6. **ÿ±ÿßÿ®ÿ∑€Å ŸÖÿπŸÑŸàŸÖÿßÿ™:** ŸÅŸàŸÜ: +92 336 1101234ÿå ÿß€å ŸÖ€åŸÑ: easternservices.pk@gmail.com (€ÅŸÖ€åÿ¥€Å €å€Å ÿßÿ≥ÿ™ÿπŸÖÿßŸÑ ⁄©ÿ±€å⁄∫!)\n",
        "7. **⁄Øÿßÿ±ŸÜŸπ€å ŸÖÿØÿ™:** ÿØ€åŸÖ⁄© ⁄©ÿß ÿπŸÑÿßÿ¨ = 5 ÿ≥ÿßŸÑÿå ÿØ€åŸÖ⁄© Ÿæÿ±ŸàŸÅŸÜ⁄Ø = 10 ÿ≥ÿßŸÑÿå ÿπÿßŸÖ ⁄©€å⁄ëŸà⁄∫ ⁄©ÿß ⁄©ŸÜŸπÿ±ŸàŸÑ = 1 ŸÖÿß€Åÿå ⁄©⁄æŸπŸÖŸÑ ⁄©ÿß ÿπŸÑÿßÿ¨ = 3 ŸÖÿß€Å\n",
        "\n",
        "**üö® ÿ±€åŸπ/ŸÇ€åŸÖÿ™ ⁄©€í ÿßÿµŸàŸÑ - ÿ®ÿßŸÑ⁄©ŸÑ ŸÅÿßŸÑŸà ⁄©ÿ±€å⁄∫:**\n",
        "- ‚ùå ÿ±€åŸπ/ŸÇ€åŸÖÿ™ ⁄©ÿ®⁄æ€å ŸÜ€Å ÿ®ÿ™ÿßÿ¶€å⁄∫ ÿ¨ÿ® ÿ™⁄© ÿ≥ŸàÿßŸÑ ŸÖ€å⁄∫ €å€Å ŸÜ€Å €ÅŸà: ÿ±€åŸπÿå ŸÇ€åŸÖÿ™ÿå ŸÑÿß⁄Øÿ™ÿå ⁄Üÿßÿ±ÿ¨ÿ≤ÿå ⁄©ÿ™ŸÜÿßÿå ⁄©ÿ™ŸÜ€å\n",
        "- ‚ùå ÿ±ŸÇÿ®€Å ⁄©ÿ®⁄æ€å ŸÜ€Å ŸæŸà⁄Ü⁄æ€å⁄∫ ÿ¨ÿ® ÿ™⁄© ÿµÿßÿ±ŸÅ ŸÜ€í ⁄©€å⁄ë€í ⁄©ÿß ŸÖÿ≥ÿ¶ŸÑ€Å + ŸÑÿß⁄Øÿ™ ⁄©ÿß ÿ≠ÿ≥ÿßÿ® ÿØŸàŸÜŸà⁄∫ ŸÜ€Å ÿ®ÿ™ÿß€åÿß €ÅŸà\n",
        "- ‚úÖ ÿß⁄Øÿ± \"⁄©€åÿß ÿ≥ÿ±Ÿàÿ≥ÿ≤ €Å€å⁄∫ÿü\" ŸæŸà⁄Ü⁄æÿßÿå ÿ™Ÿà ÿµÿ±ŸÅ ÿ≥ÿ±Ÿàÿ≥ÿ≤ ÿ®ÿ™ÿßÿ¶€å⁄∫ - ÿ±€åŸπ ŸÜ€Å€å⁄∫\n",
        "- ‚úÖ ÿß⁄Øÿ± ÿ∑ÿ±€åŸÇ€Å/ÿπŸÖŸÑ ŸæŸà⁄Ü⁄æÿßÿå ÿ™Ÿà ÿµÿ±ŸÅ ÿπŸÖŸÑ ÿ®ÿ™ÿßÿ¶€å⁄∫ - ÿ±€åŸπ ŸÜ€Å€å⁄∫\n",
        "- ‚úÖ ÿß⁄Øÿ± ⁄Øÿßÿ±ŸÜŸπ€å ŸæŸà⁄Ü⁄æ€åÿå ÿ™Ÿà ÿµÿ±ŸÅ ⁄Øÿßÿ±ŸÜŸπ€å ⁄©€å ŸÖÿØÿ™ ÿ®ÿ™ÿßÿ¶€å⁄∫ - ÿ±€åŸπ ŸÜ€Å€å⁄∫\n",
        "\n",
        "**ÿ≠ÿ≥ÿßÿ® ⁄©€í ÿßÿµŸàŸÑ (ÿµÿ±ŸÅ ÿß⁄Øÿ± ÿµÿßÿ±ŸÅ ŸÑÿß⁄Øÿ™/ÿ±€åŸπ ŸæŸà⁄Ü⁄æ€í):**\n",
        "- ÿß⁄Øÿ± ÿµÿßÿ±ŸÅ ÿ±ŸÇÿ®€Å ÿ®ÿ™ÿßÿ¶€í (ŸÖÿ±ŸÑ€Åÿå ⁄©ŸÜÿßŸÑÿå ŸÖÿ±ÿ®ÿπ ŸÅŸπ)ÿå ÿ™Ÿà ⁄©ŸÑ ŸÑÿß⁄Øÿ™ ⁄©ÿß ÿ≠ÿ≥ÿßÿ® ŸÑ⁄Øÿßÿ¶€å⁄∫\n",
        "- 1 ŸÖÿ±ŸÑ€Å = 225 ŸÖÿ±ÿ®ÿπ ŸÅŸπÿå 1 ⁄©ŸÜÿßŸÑ = 20 ŸÖÿ±ŸÑ€Å = 4,500 ŸÖÿ±ÿ®ÿπ ŸÅŸπ\n",
        "- ŸÅÿßÿ±ŸÖŸàŸÑÿß: ÿ±ŸÇÿ®€Å (ŸÖÿ±ÿ®ÿπ ŸÅŸπ) √ó ÿ±€åŸπ ŸÅ€å ŸÖÿ±ÿ®ÿπ ŸÅŸπ = ⁄©ŸÑ ŸÑÿß⁄Øÿ™\n",
        "- ŸÖÿ´ÿßŸÑ: 10 ŸÖÿ±ŸÑ€Å = 2,250 ŸÖÿ±ÿ®ÿπ ŸÅŸπ √ó Rs. 80/ŸÖÿ±ÿ®ÿπ ŸÅŸπ = Rs. 180,000\n",
        "\n",
        "**ÿß⁄Øÿ± ÿµÿßÿ±ŸÅ ÿ±ŸÇÿ®€Å ŸÜ€Å€å⁄∫ ÿ®ÿ™ÿßÿ¶€í ÿßŸàÿ± ÿ±€åŸπ ŸæŸà⁄Ü⁄æ€í:**\n",
        "‚úÖ ÿµÿ≠€åÿ≠: \"ÿØ€åŸÖ⁄© ⁄©ÿß ÿ±€åŸπ Rs. 80 ŸÅ€å ŸÖÿ±ÿ®ÿπ ŸÅŸπ €Å€í€î\"\n",
        "‚ùå ÿ∫ŸÑÿ∑: \"ÿ®ÿ±ÿß€Å ⁄©ÿ±ŸÖ ÿßŸæŸÜÿß ÿ±ŸÇÿ®€Å ÿ®ÿ™ÿßÿ¶€å⁄∫ ÿ™ÿß⁄©€Å ŸÖ€å⁄∫ ⁄©ŸÑ ŸÑÿß⁄Øÿ™ ÿ®ÿ™ÿß ÿ≥⁄©Ÿà⁄∫€î\"\n",
        "‚ùå ÿ∫ŸÑÿ∑: \"ŸÑ€å⁄©ŸÜ ŸÅ€å ŸÖÿ±ÿ®ÿπ ŸÅŸπ ÿ±€åŸπ €å€Å €Å€í€î\"\n",
        "\n",
        "**üö® ŸÖÿ∑ŸÑŸÇ Ÿæÿßÿ®ŸÜÿØ€å - ⁄©ÿ®⁄æ€å €å€Å ŸÜ€Å ⁄©ÿ±€å⁄∫:**\n",
        "- ‚ùå ⁄©ÿ®⁄æ€å ŸÜ€Å ⁄©€Å€å⁄∫ \"ÿß⁄Øÿ± ÿ¢Ÿæ...\"\n",
        "- ‚ùå ⁄©ÿ®⁄æ€å ŸÜ€Å ⁄©€Å€å⁄∫ \"ÿßŸæŸÜÿß ÿ±ŸÇÿ®€Å ÿ®ÿ™ÿßÿ¶€å⁄∫...\"\n",
        "- ‚ùå ⁄©ÿ®⁄æ€å ŸÜ€Å ⁄©€Å€å⁄∫ \"ŸÖ€å⁄∫ ÿ¢Ÿæ ⁄©Ÿà ÿ®ÿ™ÿß ÿ≥⁄©ÿ™ÿß €ÅŸà⁄∫ ÿß⁄Øÿ±...\"\n",
        "- ‚ùå ⁄©ÿ®⁄æ€å ÿ≥ŸàÿßŸÑ ŸàÿßŸæÿ≥ ŸÜ€Å ŸæŸà⁄Ü⁄æ€å⁄∫\n",
        "\n",
        "**ÿµÿ≠€åÿ≠ ÿ¨Ÿàÿßÿ®ÿßÿ™ ⁄©€å ŸÖÿ´ÿßŸÑ€å⁄∫:**\n",
        "Q: \"ÿ¢Ÿæ ⁄©ŸàŸÜ ⁄©ŸàŸÜ ÿ≥€å ÿ≥ÿ±Ÿàÿ≥ÿ≤ ŸÅÿ±ÿß€ÅŸÖ ⁄©ÿ±ÿ™€í €Å€å⁄∫ÿü\"\n",
        "A: \"€ÅŸÖ €å€Å ÿ≥ÿ±Ÿàÿ≥ÿ≤ ŸÅÿ±ÿß€ÅŸÖ ⁄©ÿ±ÿ™€í €Å€å⁄∫: ÿØ€åŸÖ⁄© ⁄©ÿß ÿπŸÑÿßÿ¨ÿå ÿØ€åŸÖ⁄© Ÿæÿ±ŸàŸÅŸÜ⁄Øÿå ÿπÿßŸÖ ⁄©€å⁄ëŸà⁄∫ ⁄©ÿß ⁄©ŸÜŸπÿ±ŸàŸÑÿå ⁄©⁄æŸπŸÖŸÑ ⁄©ÿß ÿπŸÑÿßÿ¨ÿå ŸÅ€åŸàŸÖ€å⁄Ø€åÿ¥ŸÜÿå ⁄ÜŸà€ÅŸà⁄∫ ⁄©ÿß ⁄©ŸÜŸπÿ±ŸàŸÑÿå ÿßŸàÿ± ⁄©ÿß⁄©ÿ±Ÿà⁄Ü ⁄©ŸÜŸπÿ±ŸàŸÑ€î\"\n",
        "\n",
        "Q: \"ÿØ€åŸÖ⁄© ⁄©€í ÿπŸÑÿßÿ¨ ⁄©ÿß ÿ∑ÿ±€åŸÇ€Å ⁄©€åÿß €Å€íÿü\"\n",
        "A: \"€ÅŸÖ Ÿæÿ±ÿßŸæÿ±Ÿπ€å ⁄©ÿß ŸÖÿπÿßÿ¶ŸÜ€Å ⁄©ÿ±ÿ™€í €Å€å⁄∫ÿå ÿ®ŸÜ€åÿßÿØ ⁄©€í ÿßÿ±ÿØ ⁄Øÿ±ÿØ ÿ≥Ÿàÿ±ÿßÿÆ ⁄©ÿ±ÿ™€í €Å€å⁄∫ÿå ŸÖŸπ€å ŸÖ€å⁄∫ Ÿπÿ±ŸÖ€åŸπ€åÿ≥ÿßÿ¶€å⁄à ÿßŸÜÿ¨€å⁄©Ÿπ ⁄©ÿ±ÿ™€í €Å€å⁄∫ÿå ÿßŸàÿ± ÿ™ŸÖÿßŸÖ ŸÑ⁄©⁄ë€å ⁄©€å ÿ≥ÿßÿÆÿ™Ÿà⁄∫ ⁄©ÿß ÿπŸÑÿßÿ¨ ⁄©ÿ±ÿ™€í €Å€å⁄∫€î 5 ÿ≥ÿßŸÑ ⁄©€å ⁄Øÿßÿ±ŸÜŸπ€å €Å€í€î\"\n",
        "\n",
        "Q: \"⁄©€åÿß ÿπŸÑÿßÿ¨ ⁄©€í ÿØŸàÿ±ÿßŸÜ ⁄Ø⁄æÿ± ÿÆÿßŸÑ€å ⁄©ÿ±ŸÜÿß €ÅŸà⁄Øÿßÿü\"\n",
        "A: \"ŸÅ€åŸàŸÖ€å⁄Ø€åÿ¥ŸÜ ⁄©€í ŸÑ€å€í €Åÿß⁄∫ - 24-48 ⁄Ø⁄æŸÜŸπ€í ⁄©€í ŸÑ€å€í ÿÆÿßŸÑ€å ⁄©ÿ±ŸÜÿß €ÅŸà⁄Øÿß€î ÿØ€åŸÖ⁄© ⁄©€í ÿπŸÑÿßÿ¨ ⁄©€í ŸÑ€å€í ÿ¢Ÿæ ÿ±€Å ÿ≥⁄©ÿ™€í €Å€å⁄∫ ŸÑ€å⁄©ŸÜ ÿπŸÑÿßÿ¨ ÿ¥ÿØ€Å ÿπŸÑÿßŸÇŸà⁄∫ ÿ≥€í 4-6 ⁄Ø⁄æŸÜŸπ€í ÿØŸàÿ± ÿ±€Å€å⁄∫€î\"\n",
        "\n",
        "**ÿµŸÜŸÅ ⁄©€í ÿßÿµŸàŸÑ (ŸÖÿ∞⁄©ÿ±/ŸÖÿ§ŸÜÿ´):**\n",
        "- ŸÖÿ∞⁄©ÿ± ÿßÿ≥ÿ™ÿπŸÖÿßŸÑ ⁄©ÿ±€å⁄∫: ÿØ€åŸÖ⁄©ÿå ⁄©⁄æŸπŸÖŸÑÿå ÿ≥ÿ±Ÿàÿ≥ÿå ÿπŸÑÿßÿ¨ÿå ⁄Øÿßÿ±ŸÜŸπ€å\n",
        "  ‚úÖ \"ÿØ€åŸÖ⁄© ⁄©ÿß ÿ±€åŸπ\" (ŸÜ€Å ⁄©€Å \"⁄©€å ÿ±€åŸπ\")\n",
        "  ‚úÖ \"ÿ≥ÿ±Ÿàÿ≥ ⁄©€å ⁄Øÿßÿ±ŸÜŸπ€å\" (ŸÜ€Å ⁄©€Å \"⁄©ÿß ⁄Øÿßÿ±ŸÜŸπ€å\")\n",
        "  ‚úÖ \"ÿπŸÑÿßÿ¨ ⁄©€å ŸÑÿß⁄Øÿ™\" (ŸÜ€Å ⁄©€Å \"⁄©ÿß ŸÑÿß⁄Øÿ™\")\n",
        "\n",
        "ÿ¨Ÿàÿßÿ® (ÿ≥€åÿØ⁄æÿßÿå ⁄©Ÿàÿ¶€å ÿßÿ∂ÿßŸÅ€å ŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÜ€Å€å⁄∫):\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# CREATE RAG CHAIN FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def create_rag_chain(language='english'):\n",
        "    \"\"\"Create RAG chain with language-specific prompt\"\"\"\n",
        "\n",
        "    if language == 'urdu':\n",
        "        prompt_template = URDU_PROMPT\n",
        "    elif language == 'roman_urdu':\n",
        "        prompt_template = ROMAN_URDU_PROMPT\n",
        "    else:\n",
        "        prompt_template = ENGLISH_PROMPT\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    chain = (\n",
        "        {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    return chain\n",
        "\n",
        "print(\"‚úÖ ULTIMATE Language-specific prompts created!\")\n",
        "print(\"   üö® NEW RULE: Answer ONLY what is asked - NO rate obsession!\")\n",
        "print(\"=\"*80)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU4L3ivxIG7A",
        "outputId": "6a3d7c21-5179-4cbe-8409-f06caac8462b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ULTIMATE Language-specific prompts created!\n",
            "   üö® NEW RULE: Answer ONLY what is asked - NO rate obsession!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 10 - Fallback Mechanism"
      ],
      "metadata": {
        "id": "_96-kq-SIWkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: ULTIMATE Fallback Mechanism (URDU KEYWORD BOOSTING)\n",
        "\n",
        "FALLBACK_RESPONSES = {\n",
        "    'english': \"\"\"I apologize, but I don't have specific information about that in my current knowledge base.\n",
        "\n",
        "However, I can help you with:\n",
        "- Termite control and treatment\n",
        "- Bed bug elimination\n",
        "- Cockroach, rodent, and mosquito control\n",
        "- Service pricing and packages\n",
        "- Booking and scheduling\n",
        "\n",
        "For immediate assistance, please contact Eastern Services:\n",
        "üìû Phone: +92 336 1101234\n",
        "üìß Email: easternservices.pk@gmail.com\n",
        "\n",
        "Would you like to know about any of these services?\"\"\",\n",
        "\n",
        "    'roman_urdu': \"\"\"Maafi chahta hoon, lekin mere paas is baray mein specific information nahi hai.\n",
        "\n",
        "Main aapki in cheezon mein madad kar sakta hoon:\n",
        "- Termite (deemak) control aur treatment\n",
        "- Bed bugs (khatmal) ka khatma\n",
        "- Cockroach, rats, aur mosquito control\n",
        "- Service rates aur packages\n",
        "- Booking aur scheduling\n",
        "\n",
        "Foran madad ke liye Eastern Services se contact karein:\n",
        "üìû Phone: +92 336 1101234\n",
        "üìß Email: easternservices.pk@gmail.com\n",
        "\n",
        "Kya aap in services ke baray mein jaanna chahenge?\"\"\",\n",
        "\n",
        "    'urdu': \"\"\"ŸÖÿπÿ∞ÿ±ÿ™ÿå ŸÑ€å⁄©ŸÜ ŸÖ€åÿ±€í Ÿæÿßÿ≥ ÿßÿ≥ ÿ®ÿßÿ±€í ŸÖ€å⁄∫ ŸÖÿÆÿµŸàÿµ ŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÜ€Å€å⁄∫ €Å€å⁄∫€î\n",
        "\n",
        "ŸÖ€å⁄∫ ÿ¢Ÿæ ⁄©€å ÿßŸÜ ⁄Ü€åÿ≤Ÿà⁄∫ ŸÖ€å⁄∫ ŸÖÿØÿØ ⁄©ÿ± ÿ≥⁄©ÿ™ÿß €ÅŸà⁄∫:\n",
        "- ÿØ€åŸÖ⁄© ⁄©ŸÜŸπÿ±ŸàŸÑ ÿßŸàÿ± ÿπŸÑÿßÿ¨\n",
        "- ⁄©⁄æŸπŸÖŸÑ ⁄©ÿß ÿÆÿßÿ™ŸÖ€Å\n",
        "- ⁄©ÿß⁄©ÿ±Ÿà⁄Üÿå ⁄ÜŸà€Å€íÿå ÿßŸàÿ± ŸÖ⁄Ü⁄æÿ± ⁄©ŸÜŸπÿ±ŸàŸÑ\n",
        "- ÿ≥ÿ±Ÿàÿ≥ ⁄©€å ŸÇ€åŸÖÿ™€å⁄∫ ÿßŸàÿ± Ÿæ€å⁄©ÿ¨ÿ≤\n",
        "- ÿ®⁄©ŸÜ⁄Ø ÿßŸàÿ± ÿ¥€å⁄àŸàŸÑŸÜ⁄Ø\n",
        "\n",
        "ŸÅŸàÿ±€å ŸÖÿØÿØ ⁄©€í ŸÑ€å€í Eastern Services ÿ≥€í ÿ±ÿßÿ®ÿ∑€Å ⁄©ÿ±€å⁄∫:\n",
        "üìû ŸÅŸàŸÜ: +92 336 1101234\n",
        "üìß ÿß€å ŸÖ€åŸÑ: easternservices.pk@gmail.com\n",
        "\n",
        "⁄©€åÿß ÿ¢Ÿæ ÿßŸÜ ÿÆÿØŸÖÿßÿ™ ⁄©€í ÿ®ÿßÿ±€í ŸÖ€å⁄∫ ÿ¨ÿßŸÜŸÜÿß ⁄Üÿß€Å€å⁄∫ ⁄Ø€íÿü\"\"\"\n",
        "}\n",
        "\n",
        "def check_retrieval_quality(docs, question):\n",
        "    \"\"\"Check if retrieved documents are relevant (URDU KEYWORD BOOSTING).\"\"\"\n",
        "    if not docs or len(docs) == 0:\n",
        "        return False, 0.0\n",
        "\n",
        "    question_lower = question.lower()\n",
        "\n",
        "    # Extract question words\n",
        "    question_words = set(question_lower.split())\n",
        "\n",
        "    # Common stop words (expanded for multilingual)\n",
        "    common_words = {\n",
        "        'the', 'a', 'an', 'is', 'are', 'what', 'how', 'when', 'where',\n",
        "        'kya', 'hai', 'ka', 'ki', 'ke', 'mein', 'se', 'ko', 'ne', 'par',\n",
        "        'tay', 'da', 'di', 'nu', 'te', 'vich', 'wich', 'ah', 'oye',\n",
        "        'my', 'your', 'his', 'her', 'their', 'our', 'this', 'that',\n",
        "        'mery', 'meray', 'tera', 'tuhadaha', 'tussin', 'ma', 'tusi',\n",
        "        'will', 'ga', 'gi', 'gay', 'hoga', 'hogi', 'hogay', 'ho'\n",
        "    }\n",
        "\n",
        "    # Important pest control keywords (multilingual + transliterated Urdu)\n",
        "    important_keywords = {\n",
        "        # Pests\n",
        "        'deemak', 'termite', 'bed', 'bug', 'khatmal', 'cockroach',\n",
        "        'mosquito', 'rat', 'rodent', 'ant', 'chiti', 'machar', 'chooha', 'chuhay',\n",
        "        # Services\n",
        "        'treatment', 'treatement', 'control', 'spray', 'fumigation',\n",
        "        'inspection', 'service', 'ilaj', 'ilaaj',\n",
        "        # Pricing\n",
        "        'rate', 'price', 'cost', 'fee', 'qeemat', 'kimat', 'rupay', 'rupees',\n",
        "        'marla', 'kanal', 'sq', 'square', 'per',\n",
        "        # Time/Warranty\n",
        "        'guarantee', 'warranty', 'time', 'waqt', 'kitni', 'kitna',\n",
        "        # Areas\n",
        "        'ghar', 'house', 'home', 'area', 'construction', 'wood', 'kaar', 'lakri',\n",
        "        # Transliterated Urdu (from transliterate_urdu_to_roman function)\n",
        "        'meray', 'mery', 'residence', 'white', 'ant', 'bedbug', 'mattress',\n",
        "        'roach', 'kitchen', 'pest', 'dengue', 'malaria', 'mouse', 'sugar',\n",
        "        'assurance', 'much', 'many', 'timber', 'building', 'tameer'\n",
        "    }\n",
        "\n",
        "    # Remove common words\n",
        "    question_words = question_words - common_words\n",
        "\n",
        "    if len(question_words) == 0:\n",
        "        return True, 1.0\n",
        "\n",
        "    # Count keyword matches\n",
        "    important_matches = sum(1 for word in question_words if word in important_keywords)\n",
        "\n",
        "    # Calculate relevance\n",
        "    total_matches = 0\n",
        "    for doc in docs:\n",
        "        # Handle both dict and object formats\n",
        "        if isinstance(doc, dict):\n",
        "            doc_text = doc['page_content'].lower()\n",
        "        else:\n",
        "            doc_text = doc.page_content.lower()\n",
        "\n",
        "        # Count regular word matches\n",
        "        matches = sum(1 for word in question_words if word in doc_text)\n",
        "        total_matches += matches\n",
        "\n",
        "    # Calculate base relevance score\n",
        "    if len(question_words) > 0 and len(docs) > 0:\n",
        "        relevance_score = total_matches / (len(question_words) * len(docs))\n",
        "    else:\n",
        "        relevance_score = 0.0\n",
        "\n",
        "    # BOOST score if important keywords are present\n",
        "    if important_matches > 0:\n",
        "        keyword_boost = important_matches * 0.25  # INCREASED from 0.20 to 0.25\n",
        "        relevance_score = min(1.0, relevance_score + keyword_boost)\n",
        "\n",
        "    # SPECIAL HANDLING FOR URDU: Lower threshold + Extra boost\n",
        "    # Detect if query has Urdu script\n",
        "    import re\n",
        "    urdu_pattern = r'[\\u0600-\\u06FF]'\n",
        "    has_urdu_script = re.search(urdu_pattern, question)\n",
        "\n",
        "    if has_urdu_script:\n",
        "        # For Urdu queries, add EXTRA BOOST (0.15) to compensate for transliteration loss\n",
        "        relevance_score = min(1.0, relevance_score + 0.15)\n",
        "        # Use VERY LOW threshold (0.08 instead of 0.15)\n",
        "        threshold = 0.08\n",
        "    else:\n",
        "        # For Roman/English queries, use standard threshold\n",
        "        threshold = 0.15\n",
        "\n",
        "    return relevance_score >= threshold, relevance_score\n",
        "\n",
        "print(\"‚úÖ Fallback mechanism loaded!\")\n",
        "print(\"   Improved multilingual relevance checking enabled!\")\n",
        "print(\"   ‚≠ê URDU BOOST: +0.15 bonus + 0.08 threshold (vs. 0.15 for others)\")\n",
        "print(\"=\"*80)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMEkL7qRIIq1",
        "outputId": "512abfd4-5c82-4d60-d967-c79d74a26c54"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fallback mechanism loaded!\n",
            "   Improved multilingual relevance checking enabled!\n",
            "   ‚≠ê URDU BOOST: +0.15 bonus + 0.08 threshold (vs. 0.15 for others)\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 11 - Conversation Memory (Redis + Supabase)"
      ],
      "metadata": {
        "id": "SIrRBPCuIk3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Conversation Memory with Redis and Supabase\n",
        "\n",
        "import uuid\n",
        "import time\n",
        "\n",
        "# Generate unique session ID for this Colab session\n",
        "SESSION_ID = str(uuid.uuid4())[:8]\n",
        "\n",
        "def save_to_redis(question, answer, language):\n",
        "    \"\"\"Save conversation to Redis (fast, temporary cache).\"\"\"\n",
        "    if redis_client is None:\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Store in Redis with 1-hour expiry\n",
        "        key = f\"chat:{SESSION_ID}:history\"\n",
        "        conversation_entry = {\n",
        "            'question': question,\n",
        "            'answer': answer,\n",
        "            'language': language,\n",
        "            'timestamp': time.time()\n",
        "        }\n",
        "\n",
        "        # Add to list (keep last 10 exchanges)\n",
        "        redis_client.lpush(key, json.dumps(conversation_entry))\n",
        "        redis_client.ltrim(key, 0, 9)  # Keep only last 10\n",
        "        redis_client.expire(key, 3600)  # 1 hour expiry\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Redis save failed: {e}\")\n",
        "\n",
        "def save_to_supabase(question, answer, language):\n",
        "    \"\"\"Save conversation to Supabase (permanent storage).\"\"\"\n",
        "    if SessionLocal is None:\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        db = SessionLocal()\n",
        "        conversation = ConversationHistory(\n",
        "            session_id=SESSION_ID,\n",
        "            user_question=question,\n",
        "            bot_response=answer,\n",
        "            language=language,\n",
        "            meta_data=json.dumps({'source': 'colab'})  # Changed from metadata to meta_data\n",
        "        )\n",
        "        db.add(conversation)\n",
        "        db.commit()\n",
        "        db.close()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Supabase save failed: {e}\")\n",
        "\n",
        "def get_conversation_context():\n",
        "    \"\"\"Get recent conversation history from Redis.\"\"\"\n",
        "    if redis_client is None:\n",
        "        return \"No previous conversation.\"\n",
        "\n",
        "    try:\n",
        "        key = f\"chat:{SESSION_ID}:history\"\n",
        "        history = redis_client.lrange(key, 0, 2)  # Last 3 exchanges\n",
        "\n",
        "        if not history:\n",
        "            return \"No previous conversation.\"\n",
        "\n",
        "        context = \"Previous conversation:\\n\"\n",
        "        for entry_json in reversed(history):\n",
        "            entry = json.loads(entry_json)\n",
        "            context += f\"\\nUser: {entry['question'][:100]}\\n\"\n",
        "            context += f\"Assistant: {entry['answer'][:150]}...\\n\"\n",
        "\n",
        "        return context\n",
        "    except Exception as e:\n",
        "        return \"No previous conversation.\"\n",
        "\n",
        "def clear_conversation():\n",
        "    \"\"\"Clear conversation history.\"\"\"\n",
        "    if redis_client:\n",
        "        try:\n",
        "            key = f\"chat:{SESSION_ID}:history\"\n",
        "            redis_client.delete(key)\n",
        "            print(\"‚úÖ Conversation history cleared!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Clear failed: {e}\")\n",
        "\n",
        "print(f\"‚úÖ Conversation memory initialized!\")\n",
        "print(f\"   Session ID: {SESSION_ID}\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWQ_O_YsIg78",
        "outputId": "aa283687-fcab-416e-abb4-d098405ce7f9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Conversation memory initialized!\n",
            "   Session ID: f6673e71\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 12 - Evaluation Metrics"
      ],
      "metadata": {
        "id": "sEB1J9noIyiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Evaluation Metrics\n",
        "\n",
        "evaluation_log = []\n",
        "\n",
        "def evaluate_response(question, answer, retrieved_docs, response_time, language):\n",
        "    \"\"\"Evaluate response quality.\"\"\"\n",
        "    metrics = {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'question': question[:100],\n",
        "        'answer_length': len(answer),\n",
        "        'response_time': response_time,\n",
        "        'language': language,\n",
        "        'num_docs_retrieved': len(retrieved_docs),\n",
        "        'relevance_score': 0.0,\n",
        "        'completeness_score': 0.0,\n",
        "        'overall_score': 0.0\n",
        "    }\n",
        "\n",
        "    # Relevance score\n",
        "    question_words = set(question.lower().split())\n",
        "    answer_words = set(answer.lower().split())\n",
        "    common_words = {'the', 'a', 'an', 'is', 'are', 'what', 'how', 'kya', 'hai', 'ka'}\n",
        "    question_words = question_words - common_words\n",
        "\n",
        "    if len(question_words) > 0:\n",
        "        overlap = len(question_words.intersection(answer_words))\n",
        "        metrics['relevance_score'] = min(1.0, overlap / len(question_words))\n",
        "\n",
        "    # Completeness score\n",
        "    if 50 <= len(answer) <= 500:\n",
        "        metrics['completeness_score'] = 1.0\n",
        "    elif len(answer) < 50:\n",
        "        metrics['completeness_score'] = len(answer) / 50\n",
        "    else:\n",
        "        metrics['completeness_score'] = max(0.5, 500 / len(answer))\n",
        "\n",
        "    # Overall score\n",
        "    metrics['overall_score'] = (\n",
        "        metrics['relevance_score'] * 0.6 +\n",
        "        metrics['completeness_score'] * 0.4\n",
        "    )\n",
        "\n",
        "    evaluation_log.append(metrics)\n",
        "    return metrics\n",
        "\n",
        "def display_metrics(metrics, show_details=True):\n",
        "    \"\"\"Display evaluation metrics.\"\"\"\n",
        "    if show_details:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"RESPONSE EVALUATION\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Language: {metrics['language']}\")\n",
        "        print(f\"Response Time: {metrics['response_time']:.2f}s\")\n",
        "        print(f\"Answer Length: {metrics['answer_length']} characters\")\n",
        "        print(f\"\\nScores:\")\n",
        "        print(f\"  Relevance: {metrics['relevance_score']:.2f}/1.00\")\n",
        "        print(f\"  Completeness: {metrics['completeness_score']:.2f}/1.00\")\n",
        "        print(f\"  Overall: {metrics['overall_score']:.2f}/1.00\")\n",
        "\n",
        "        if metrics['overall_score'] >= 0.8:\n",
        "            print(f\"  Quality: ‚úÖ Excellent\")\n",
        "        elif metrics['overall_score'] >= 0.6:\n",
        "            print(f\"  Quality: ‚úì Good\")\n",
        "        elif metrics['overall_score'] >= 0.4:\n",
        "            print(f\"  Quality: ‚ö†Ô∏è  Fair\")\n",
        "        else:\n",
        "            print(f\"  Quality: ‚ùå Poor\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "def get_evaluation_summary():\n",
        "    \"\"\"Get evaluation summary.\"\"\"\n",
        "    if not evaluation_log:\n",
        "        print(\"No evaluations yet!\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EVALUATION SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total Queries: {len(evaluation_log)}\")\n",
        "\n",
        "    avg_relevance = sum(e['relevance_score'] for e in evaluation_log) / len(evaluation_log)\n",
        "    avg_completeness = sum(e['completeness_score'] for e in evaluation_log) / len(evaluation_log)\n",
        "    avg_overall = sum(e['overall_score'] for e in evaluation_log) / len(evaluation_log)\n",
        "    avg_response_time = sum(e['response_time'] for e in evaluation_log) / len(evaluation_log)\n",
        "\n",
        "    print(f\"\\nAverage Scores:\")\n",
        "    print(f\"  Relevance: {avg_relevance:.2f}/1.00\")\n",
        "    print(f\"  Completeness: {avg_completeness:.2f}/1.00\")\n",
        "    print(f\"  Overall: {avg_overall:.2f}/1.00\")\n",
        "    print(f\"\\nAverage Response Time: {avg_response_time:.2f}s\")\n",
        "\n",
        "    languages = {}\n",
        "    for e in evaluation_log:\n",
        "        lang = e['language']\n",
        "        if lang not in languages:\n",
        "            languages[lang] = 0\n",
        "        languages[lang] += 1\n",
        "\n",
        "    print(f\"\\nQueries by Language:\")\n",
        "    for lang, count in languages.items():\n",
        "        print(f\"  {lang}: {count}\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "\n",
        "print(\"‚úÖ Evaluation metrics loaded!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouypi-_5Ir3Q",
        "outputId": "64762cd0-3627-4a71-dcae-c81bcd1e4ef3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Evaluation metrics loaded!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 13 - Complete Chatbot Function"
      ],
      "metadata": {
        "id": "2Kukaeu2I7KP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Complete Chatbot Function (CLEAN - NO DEBUG OUTPUT)\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "import time\n",
        "\n",
        "def show_welcome_message():\n",
        "    \"\"\"Display welcome message when chatbot initializes.\"\"\"\n",
        "    welcome_msg = \"\"\"\n",
        "# Assalam-o-Alaikum! üëã\n",
        "\n",
        "**Welcome to Eastern Services Pest Control Chatbot**\n",
        "\n",
        "Main aapki kya madad kar sakta hoon? (How may I help you?)\n",
        "\n",
        "---\n",
        "\n",
        "**Available Services:**\n",
        "- Termite Treatment & Proofing\n",
        "- Bed Bug Treatment\n",
        "- General Pest Control\n",
        "- Fumigation Services\n",
        "- Rodent Control\n",
        "- Cockroach Control\n",
        "\n",
        "**Contact Us:**\n",
        "üìû Phone: +92 336 1101234\n",
        "üìß Email: easternservices.pk@gmail.com\n",
        "\n",
        "---\n",
        "\"\"\"\n",
        "    display(Markdown(welcome_msg))\n",
        "\n",
        "def ask_chatbot(question):\n",
        "    \"\"\"\n",
        "    Complete chatbot function - CLEAN VERSION (NO DEBUG OUTPUT)\n",
        "\n",
        "    Features:\n",
        "    - PGVector retrieval (cloud-based)\n",
        "    - Fallback mechanism\n",
        "    - Conversation memory (Redis + Supabase) - FOR LOGGING ONLY\n",
        "    - Multilingual support\n",
        "\n",
        "    Returns only the response text - no debug info, no metrics, no scores.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Detect language and greeting\n",
        "    language = detect_language(question)\n",
        "    greeting_type = detect_greeting(question)\n",
        "\n",
        "    try:\n",
        "        # Retrieve relevant documents from PGVector\n",
        "        docs = retrieve_similar_chunks(question, k=3)\n",
        "\n",
        "        # Check retrieval quality\n",
        "        is_relevant, score = check_retrieval_quality(docs, question)\n",
        "\n",
        "        # If retrieval quality is poor, use fallback\n",
        "        if not is_relevant:\n",
        "            fallback_msg = FALLBACK_RESPONSES.get(language, FALLBACK_RESPONSES['english'])\n",
        "\n",
        "            # Add greeting if user greeted\n",
        "            if greeting_type:\n",
        "                if greeting_type == 'urdu_greeting':\n",
        "                    fallback_msg = \"ŸàÿπŸÑŸäŸÉŸÖ ÿßŸÑÿ≥ŸÑÿßŸÖ Ÿàÿ±ÿ≠ŸÖÿ© ÿßŸÑŸÑŸá Ÿàÿ®ÿ±ŸÉÿßÿ™Ÿá\\n\\n\" + fallback_msg\n",
        "                elif greeting_type == 'roman_urdu_greeting':\n",
        "                    fallback_msg = \"Waalaikum Assalam wa Rahmatullahi wa Barakatuh\\n\\n\" + fallback_msg\n",
        "                elif greeting_type == 'english_greeting':\n",
        "                    fallback_msg = \"Hello! \" + fallback_msg\n",
        "\n",
        "            # Display response (CLEAN - NO DEBUG)\n",
        "            display(Markdown(fallback_msg))\n",
        "\n",
        "            # Save to memory (FOR LOGGING ONLY)\n",
        "            save_to_redis(question, fallback_msg, language)\n",
        "            save_to_supabase(question, fallback_msg, language)\n",
        "\n",
        "            return\n",
        "\n",
        "        # Create chain and get response\n",
        "        chain = create_rag_chain(language)\n",
        "\n",
        "        # Format docs for chain\n",
        "        context = \"\\n\\n\".join([doc['page_content'] if isinstance(doc, dict) else doc.page_content for doc in docs])\n",
        "\n",
        "        # Get response from LLM\n",
        "        response = chain.invoke({\"context\": context, \"question\": question})\n",
        "        response = clean_response(response, language, question)\n",
        "\n",
        "        # Add greeting if user greeted\n",
        "        if greeting_type:\n",
        "            if greeting_type == 'urdu_greeting':\n",
        "                response = \"ŸàÿπŸÑŸäŸÉŸÖ ÿßŸÑÿ≥ŸÑÿßŸÖ Ÿàÿ±ÿ≠ŸÖÿ© ÿßŸÑŸÑŸá Ÿàÿ®ÿ±ŸÉÿßÿ™Ÿá\\n\\n\" + response\n",
        "            elif greeting_type == 'roman_urdu_greeting':\n",
        "                response = \"Waalaikum Assalam wa Rahmatullahi wa Barakatuh\\n\\n\" + response\n",
        "            elif greeting_type == 'english_greeting':\n",
        "                response = \"Hello! \" + response\n",
        "\n",
        "        # Display response (CLEAN - NO DEBUG, NO METRICS, NO SCORES)\n",
        "        display(Markdown(response))\n",
        "\n",
        "        # Save to memory (FOR LOGGING ONLY - NOT FOR LLM CONTEXT)\n",
        "        save_to_redis(question, response, language)\n",
        "        save_to_supabase(question, response, language)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = \"Maafi chahta hoon, kuch technical issue ho raha hai. Kya aap phir se try kar sakte hain?\"\n",
        "        if language == 'urdu':\n",
        "            error_msg = \"ŸÖÿπÿ∞ÿ±ÿ™ÿå ⁄©⁄Ü⁄æ ÿ™⁄©ŸÜ€å⁄©€å ŸÖÿ≥ÿ¶ŸÑ€Å €ÅŸà ÿ±€Åÿß €Å€í€î ⁄©€åÿß ÿ¢Ÿæ Ÿæ⁄æÿ± ÿ≥€í ⁄©Ÿàÿ¥ÿ¥ ⁄©ÿ± ÿ≥⁄©ÿ™€í €Å€å⁄∫ÿü\"\n",
        "        elif language == 'english':\n",
        "            error_msg = \"Sorry, there's a technical issue. Please try again.\"\n",
        "\n",
        "        display(Markdown(error_msg))\n",
        "        print(f\"‚ö†Ô∏è Error: {str(e)[:100]}\")  # Only print error to console, not to user\n",
        "\n",
        "# Show welcome message when this cell runs\n",
        "show_welcome_message()\n",
        "\n",
        "print(\"‚úÖ Chatbot ready! Ask your question using: ask_chatbot('your question')\")\n",
        "print(\"=\"*80)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "XfAq9WLGI3WL",
        "outputId": "78d145a4-113e-4f28-c129-93142629356f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n# Assalam-o-Alaikum! üëã\n\n**Welcome to Eastern Services Pest Control Chatbot**\n\nMain aapki kya madad kar sakta hoon? (How may I help you?)\n\n---\n\n**Available Services:**\n- Termite Treatment & Proofing\n- Bed Bug Treatment\n- General Pest Control\n- Fumigation Services\n- Rodent Control\n- Cockroach Control\n\n**Contact Us:**\nüìû Phone: +92 336 1101234  \nüìß Email: easternservices.pk@gmail.com\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Chatbot ready! Ask your question using: ask_chatbot('your question')\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 14 - Testing"
      ],
      "metadata": {
        "id": "qjvMQqajJ1ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: Test the Complete Cloud-Based System\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TESTING COMPLETE CLOUD-BASED RAG SYSTEM\")\n",
        "print(\"=\"*80)\n",
        "print(\"Data Source: GitHub Repository\")\n",
        "print(\"Embeddings: Supabase PGVector + Cohere\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Test various scenarios\n",
        "ask_chatbot(\"Mery ghar mein deemak hai,mera 5 marlay ka ghar hai ,to us ka rate kia ho ga?\")\n",
        "# ask_chatbot(\"Guarantee kitni hai?\", show_debug=True, show_metrics=True)\n",
        "# ask_chatbot(\"What is the bed bug treatment process?\", show_debug=True, show_metrics=True)\n",
        "# ask_chatbot(\"Tusi or keri keri service provide karday o?\", show_debug=True, show_metrics=True)\n",
        "# Test 5: PURE URDU (Termite Rate)\n",
        "# ask_chatbot(\"ŸÖ€åÿ±€í ⁄Ø⁄æÿ± ŸÖ€å⁄∫ ÿØ€åŸÖ⁄© €Å€íÿå ÿ±€åŸπ ⁄©€åÿß €ÅŸà ⁄Øÿßÿü\", show_debug=True, show_metrics=True)\n",
        "\n",
        "# Test 6: PURE URDU (Guarantee)\n",
        "# ask_chatbot(\"⁄Øÿßÿ±ŸÜŸπ€å ⁄©ÿ™ŸÜ€å €Å€íÿü\", show_debug=True, show_metrics=True)\n",
        "# ask_chatbot(\"ÿØ€åŸÖ⁄© ⁄©€í ÿπŸÑÿßÿ¨ ⁄©ÿß ÿ∑ÿ±€åŸÇ€Å ⁄©€åÿß €Å€íÿü\", show_debug=True, show_metrics=True)\n",
        "# ask_chatbot(\"ŸÅ€åŸàŸÖ€å⁄Ø€åÿ¥ŸÜ ⁄©€åÿ≥€í €ÅŸàÿ™ÿß €Å€íÿü\", show_debug=True, show_metrics=True)\n",
        "# ask_chatbot(\"⁄©€åÿß ÿ¢ÿ¨ €Å€å ÿ≥ÿ±Ÿàÿ≥ ŸÖŸÑ ÿ≥⁄©ÿ™€å €Å€íÿü\", show_debug=True, show_metrics=True)\n",
        "# ask_chatbot(\"⁄©€åÿß ⁄©Ÿàÿ¶€å ⁄àÿ≥⁄©ÿßÿ§ŸÜŸπ €Å€íÿü\", show_debug=True, show_metrics=True)\n",
        "ask_chatbot(\"⁄©€åÿß ÿπŸÑÿßÿ¨ ⁄©€í ÿØŸàÿ±ÿßŸÜ ⁄Ø⁄æÿ± ÿÆÿßŸÑ€å ⁄©ÿ±ŸÜÿß €ÅŸà⁄Øÿßÿü\")\n",
        "\n",
        "# Show evaluation summary\n",
        "get_evaluation_summary()\n",
        "\n",
        "print(\"\\n‚úÖ Cloud-based system testing complete!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "wEdWOhQMJAXE",
        "outputId": "090186f4-5c4f-4b78-c0bc-29d3f117f060"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "TESTING COMPLETE CLOUD-BASED RAG SYSTEM\n",
            "================================================================================\n",
            "Data Source: GitHub Repository\n",
            "Embeddings: Supabase PGVector + Cohere\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Deemak ka rate Rs. 80 per sq ft hai. Apka 5 marlay ka ghar hai, lekin humein area sq ft mein maloom hona chahiye. 1 Marla = 225 sq ft, to apka 5 marlay ka ghar = 5 √ó 225 = 1125 sq ft. Total cost = 1125 sq ft √ó Rs. 80/sq ft = Rs. 90,000. Humari guarantee period 1 saal hai. Agar aapko aur information chahiye, to humse contact karein: Phone: +92 336 1101234, Email: easternservices.pk@gmail.com."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   üîÑ Transliterated: '⁄©€åÿß ÿπŸÑÿßÿ¨ ⁄©€í ÿØŸàÿ±ÿßŸÜ ⁄Ø⁄æÿ± ÿÆÿßŸÑ€å ⁄©ÿ±ŸÜÿß €ÅŸà⁄Øÿßÿü...' ‚Üí 'kya kia what ilaj treatment control serv...'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "ŸÅ€åŸàŸÖ€å⁄Ø€åÿ¥ŸÜ ⁄©€í ŸÑ€å€í €Åÿß⁄∫ - 24-48 ⁄Ø⁄æŸÜŸπ€í ⁄©€í ŸÑ€å€í ÿÆÿßŸÑ€å ⁄©ÿ±ŸÜÿß €ÅŸà⁄Øÿß€î ÿØ€åŸÖ⁄© ⁄©€í ÿπŸÑÿßÿ¨ ⁄©€í ŸÑ€å€í ÿ¢Ÿæ ÿ±€Å ÿ≥⁄©ÿ™€í €Å€å⁄∫ ŸÑ€å⁄©ŸÜ ÿπŸÑÿßÿ¨ ÿ¥ÿØ€Å ÿπŸÑÿßŸÇŸà⁄∫ ÿ≥€í 4-6 ⁄Ø⁄æŸÜŸπ€í ÿØŸàÿ± ÿ±€Å€å⁄∫€î"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "EVALUATION SUMMARY\n",
            "============================================================\n",
            "Total Queries: 1\n",
            "\n",
            "Average Scores:\n",
            "  Relevance: 0.46/1.00\n",
            "  Completeness: 1.00/1.00\n",
            "  Overall: 0.68/1.00\n",
            "\n",
            "Average Response Time: 3.26s\n",
            "\n",
            "Queries by Language:\n",
            "  roman_urdu: 1\n",
            "============================================================\n",
            "\n",
            "‚úÖ Cloud-based system testing complete!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 15 - Usage Instructions"
      ],
      "metadata": {
        "id": "2wSm1KKAUFry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üéâ System Ready!\n",
        "\n",
        "## How to Use:\n",
        "\n",
        "### Basic Query:\n",
        "ask_chatbot(\"meray kaar cha wood da kam oye ah ,tay deemak bohat lagi ah wood wich,tay ma oda treatement karwana chana\")\n",
        "ask_chatbot(\"Eastern services k baray may kuch batay?services kaya kaya provide karta hai?aur CEO kon hai? aur head office kidar hai\")\n",
        "ask_chatbot(\"Cockroach k baray may batay k wo kis teran k hotay hai?\")\n",
        "ask_chatbot(\"bed bug k treatement ka kaya process hai?\")\n",
        "ask_chatbot(\"pest sa hum kesay bach saktay hai ,means koi precautions bata dan.\")\n",
        "ask_chatbot(\"Termite k treatement ka kaya process hai?\")\n",
        "ask_chatbot(\"commercial k liya Monthly Plans kaya hai aur payment k baray may b bata dan\")\n",
        "ask_chatbot(\"Seasonal Pest Calendar kaya hai?\")\n",
        "ask_chatbot(\"mera 10 marlay ka ghar hai ,meray ghar may mechar bohat ,to ma fumigation karwana chahta hun ,to us ki quotation bana dan.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### With Debug Info:\n",
        "ask_chatbot(\"rate ki ah demaak treatement da\", show_debug=True, show_metrics=True)\n",
        "### Clear Conversation:\n",
        "clear_conversation()\n",
        "\n",
        "### View Evaluation Summary:\n",
        "get_evaluation_summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c_y2P2xENptN",
        "outputId": "255311b7-68ba-41af-861b-21d6ac478e31"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [üîÑ Punjabi detected ‚Üí Mapped to Roman Urdu]\n",
            "\n",
            "> QUESTION: meray kaar cha wood da kam oye ah ,tay deemak bohat lagi ah wood wich,tay ma oda treatement karwana ...\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Meray pass deemak ka ilaj karwana chahye, hum Eastern Services mein advanced pre-construction aur post-construction anti-termite treatments provide karte hain. Hum property inspect karte hain, foundation ke around holes drill karte hain, soil mein termiticide inject karte hain, aur tamam wooden structures ka treatment karte hain. 5 saal ki guarantee hai, aur aap humse contact kar sakte hain: Phone: +92 336 1101234, Email: easternservices.pk@gmail.com."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [üîÑ Punjabi detected ‚Üí Mapped to Roman Urdu]\n",
            "\n",
            "> QUESTION: Eastern services k baray may kuch batay?services kaya kaya provide karta hai?aur CEO kon hai? aur he...\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Eastern Services yeh services provide karta hai: Termite Control, Fumigation Services, Bed Bug Treatment, Cockroach Control, Rodent Control, Mosquito/Dengue Control, aur Commercial aur Residential Pest Control services. CEO ka naam Raja Zahid Iqbal hai. Head office Islamabad mein hai, Office # 12, 2nd Floor, Siddique Trade Centre, Blue Area. Phone: +92 336 1101234, Email: easternservices.pk@gmail.com."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [üîÑ Punjabi detected ‚Üí Mapped to Roman Urdu]\n",
            "\n",
            "> QUESTION: Cockroach k baray may batay k wo kis teran k hotay hai?\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Cockroach ki kai tarah hoti hain, jaise German Cockroach, American Cockroach, Oriental Cockroach, aur Brown-banded Cockroach. Ye alag alag rang aur size ke hote hain, jaise German Cockroach chhota aur light brown hota hai, jabki American Cockroach bada aur reddish-brown hota hai. Agar aapko cockroach se related koi problem hai, to humse contact karein: Phone: +92 336 1101234, Email: easternservices.pk@gmail.com."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "> QUESTION: bed bug k treatement ka kaya process hai?\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The bed bug treatment process includes an initial inspection to identify the infestation level, followed by heat treatment or chemical spray application, and treatment of mattresses, bed frames, furniture, and cracks. A second treatment is done after 15 days to kill newly hatched eggs, and a follow-up inspection is conducted after 30 days. For more information, you can contact us at Phone: +92 336 1101234 or Email: easternservices.pk@gmail.com."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [üîÑ Punjabi detected ‚Üí Mapped to Roman Urdu]\n",
            "\n",
            "> QUESTION: pest sa hum kesay bach saktay hai ,means koi precautions bata dan.\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Pest se bachne ke liye aapko apne ghar ko saaf rakhna chahiye, khana aur paani ko dhak kar rakhna chahiye, aur naliyon aur nalayon ko saaf rakhna chahiye. Aapko apne ghar mein deemak aur khatmal ki jaanch bhi karwani chahiye. Agar aapko koi problem hoti hai to aap humse contact kar sakte hain, Phone: +92 336 1101234, Email: easternservices.pk@gmail.com."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "> QUESTION: Termite k treatement ka kaya process hai?\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The termite treatment process includes inspection, drilling holes around the foundation, chemical injection, treating wooden structures, and sealing drill holes. This process takes 1-3 days depending on property size. For more information, you can contact us at +92 336 1101234 or email easternservices.pk@gmail.com."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [üîÑ Punjabi detected ‚Üí Mapped to Roman Urdu]\n",
            "\n",
            "> QUESTION: commercial k liya Monthly Plans kaya hai aur payment k baray may b bata dan\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Commercial k liye hum yeh Monthly Plans offer karte hain: Small Business, Restaurant/Food Service, Retail Store, Warehouse, aur Office Building. In plans ki pricing PKR 5,000 se lekar PKR 15,000 tak hai, jo facility ke size aur service frequency par depend karta hai. Payment ke liye aap humse +92 336 1101234 par contact kar sakte hain ya easternservices.pk@gmail.com par email kar sakte hain."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [üîÑ Punjabi detected ‚Üí Mapped to Roman Urdu]\n",
            "\n",
            "> QUESTION: Seasonal Pest Calendar kaya hai?\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Seasonal Pest Calendar hum provide karte hain. Ismein humne Pakistan ke liye month-by-month pest activity guide banaya hai, jismein winter, spring, summer, monsoon, aur phir se winter season ke liye pest activity aur prevention methods bataye gaye hain. Phone: +92 336 1101234, Email: easternservices.pk@gmail.com."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [üîÑ Punjabi detected ‚Üí Mapped to Roman Urdu]\n",
            "\n",
            "> QUESTION: mera 10 marlay ka ghar hai ,meray ghar may mechar bohat ,to ma fumigation karwana chahta hun ,to us ...\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Aapke 10 marlay ke ghar ke liye Mosquito Fogging ki quotation Rs. 3,500 hai. Agar aapko aur bhi services ki zaroorat hai, to humari services mein shamil hain: Termite Treatment, Termite Proofing, General Pest Control, Bed Bug Treatment, Fumigation, Rodent Control, aur Cockroach Control. Humse contact karne ke liye phone: +92 336 1101234, aur email: easternservices.pk@gmail.com par rabta karein."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [üîÑ Punjabi detected ‚Üí Mapped to Roman Urdu]\n",
            "\n",
            "> QUESTION: rate ki ah demaak treatement da\n",
            "  [Language: ROMAN_URDU]\n",
            "  [Session ID: a5f050b4]\n",
            "  [Relevance Score: 0.83]\n",
            "  [Using Fallback: False]\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Deemak ka rate Rs. 80 per sq ft hai. Hum 1 saal ki guarantee dete hain. Agar aapko aur information chahiye, to humse +92 336 1101234 par phone kar sakte hain ya easternservices.pk@gmail.com par email kar sakte hain."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "RESPONSE EVALUATION\n",
            "============================================================\n",
            "Language: roman_urdu\n",
            "Response Time: 11.39s\n",
            "Answer Length: 215 characters\n",
            "\n",
            "Scores:\n",
            "  Relevance: 0.33/1.00\n",
            "  Completeness: 1.00/1.00\n",
            "  Overall: 0.60/1.00\n",
            "  Quality: ‚úì Good\n",
            "============================================================\n",
            "‚úÖ Conversation history cleared!\n",
            "\n",
            "============================================================\n",
            "EVALUATION SUMMARY\n",
            "============================================================\n",
            "Total Queries: 21\n",
            "\n",
            "Average Scores:\n",
            "  Relevance: 0.37/1.00\n",
            "  Completeness: 1.00/1.00\n",
            "  Overall: 0.62/1.00\n",
            "\n",
            "Average Response Time: 9.43s\n",
            "\n",
            "Queries by Language:\n",
            "  roman_urdu: 11\n",
            "  english: 3\n",
            "  urdu: 7\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**COMPREHENSIVE TESTING - Context Memory + Accuracy**"
      ],
      "metadata": {
        "id": "nxnTohD0xRUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 16: COMPREHENSIVE TESTING - Context Memory + Accuracy\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPREHENSIVE TESTING - MEMORY + ACCURACY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Clear previous conversation\n",
        "clear_conversation()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TEST SET 1: CONTEXT MEMORY (ROMAN URDU)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Sequential conversation to test memory\n",
        "ask_chatbot(\"Aap kon kon si services provide karte hain?\", show_debug=True)\n",
        "ask_chatbot(\"Termite ka rate kya hai?\", show_debug=True)  # Should remember context\n",
        "ask_chatbot(\"Guarantee kitni hai?\", show_debug=True)  # Should know it's about termite\n",
        "\n",
        "# Clear for next test\n",
        "clear_conversation()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TEST SET 2: CONTEXT MEMORY (PUNJABI)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "ask_chatbot(\"Tussin keri keri service provide karday o?\", show_debug=True)\n",
        "ask_chatbot(\"Deemak da rate ki ah?\", show_debug=True)  # Should remember context\n",
        "ask_chatbot(\"Guarantee kitni ae?\", show_debug=True)  # Should know it's about deemak\n",
        "\n",
        "# Clear for next test\n",
        "clear_conversation()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TEST SET 3: ACCURACY - FIXED VALUES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Test specific, unchangeable values\n",
        "ask_chatbot(\"What is the termite treatment rate?\", show_debug=True)  # Should say Rs. 80/sq ft\n",
        "ask_chatbot(\"How long is the guarantee period?\", show_debug=True)  # Should say 1 year or 10 years\n",
        "ask_chatbot(\"What is the contact number?\", show_debug=True)  # Should give exact number\n",
        "ask_chatbot(\"What is the email address?\", show_debug=True)  # Should give exact email\n",
        "ask_chatbot(\"Which areas do you serve?\", show_debug=True)  # Should list cities\n",
        "\n",
        "# Clear for next test\n",
        "clear_conversation()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TEST SET 4: PURE URDU (NASTALIQ SCRIPT)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "ask_chatbot(\"ŸÖ€åÿ±€í ⁄Ø⁄æÿ± ŸÖ€å⁄∫ ÿØ€åŸÖ⁄© €Å€íÿå ÿ±€åŸπ ⁄©€åÿß €ÅŸà ⁄Øÿßÿü\", show_debug=True)\n",
        "ask_chatbot(\"⁄Øÿßÿ±ŸÜŸπ€å ⁄©ÿ™ŸÜ€å €Å€íÿü\", show_debug=True)\n",
        "ask_chatbot(\"⁄©ŸàŸÜ ⁄©ŸàŸÜ ÿ≥€å ÿÆÿØŸÖÿßÿ™ €Å€å⁄∫ÿü\", show_debug=True)\n",
        "\n",
        "# Clear for next test\n",
        "clear_conversation()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TEST SET 5: MULTILINGUAL MIX (REAL SCENARIO)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "ask_chatbot(\"What services do you offer?\", show_debug=True)  # English\n",
        "ask_chatbot(\"Deemak ka rate kya hai?\", show_debug=True)  # Switch to Roman Urdu\n",
        "ask_chatbot(\"Mera 10 marlay ka ghar hai\", show_debug=True)  # Switch to Roman Urdu\n",
        "ask_chatbot(\"Guarantee kitni ae?\", show_debug=True)  # Punjabi\n",
        "ask_chatbot(\"⁄©ŸàŸÜ ÿ≥€í ÿπŸÑÿßŸÇ€íÿü\", show_debug=True)  # Pure Urdu\n",
        "\n",
        "# Show final summary\n",
        "get_evaluation_summary()\n",
        "\n",
        "print(\"\\n‚úÖ Comprehensive testing complete!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wcW7O66PxMvs",
        "outputId": "8e34043c-7401-44c2-b705-a7aa60dc92f3"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "COMPREHENSIVE TESTING - MEMORY + ACCURACY\n",
            "================================================================================\n",
            "‚úÖ Conversation history cleared!\n",
            "\n",
            "================================================================================\n",
            "TEST SET 1: CONTEXT MEMORY (ROMAN URDU)\n",
            "================================================================================\n",
            "\n",
            "> QUESTION: Aap kon kon si services provide karte hain?\n",
            "  [Language: ROMAN_URDU]\n",
            "  [Session ID: a5f050b4]\n",
            "  [Relevance Score: 0.24]\n",
            "  [Using Fallback: False]\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hum yeh services provide karte hain: Office Pest Control Package, Restaurant and Food Service Package, Hotel and Hospitality Package, Warehouse and Storage Package, Termite Treatment, Termite Proofing, General Pest Control, Bed Bug Treatment, Fumigation, Rodent Control, aur Cockroach Control. Humara contact number hai +92 336 1101234 aur email hai easternservices.pk@gmail.com."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "> QUESTION: Termite ka rate kya hai?\n",
            "  [Language: ROMAN_URDU]\n",
            "  [Session ID: a5f050b4]\n",
            "  [Relevance Score: 1.00]\n",
            "  [Using Fallback: False]\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Termite ka rate Rs. 80 per sq ft hai. Yeh rate post-construction termite treatment ke liye hai, jo 1 saal ki guarantee ke saath aata hai. Pre-construction termite proofing ka rate Rs. 65 per sq ft hai, jo 10 saal ki guarantee ke saath aata hai."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "> QUESTION: Guarantee kitni hai?\n",
            "  [Language: ROMAN_URDU]\n",
            "  [Session ID: a5f050b4]\n",
            "  [Relevance Score: 0.83]\n",
            "  [Using Fallback: False]\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Guarantee kitni hai, yeh service ke type par depend karta hai. Termite Treatment ki guarantee 5 saal hai, Termite Proofing ki guarantee 10 saal hai, General Pest Control ki guarantee 1 mahina hai, Bed Bug Treatment ki guarantee 3 mahine hai, Rodent Control ki guarantee 2 mahine hai, aur Cockroach Control ki guarantee 2 mahine hai. Agar aapko aur details chahiye, to humse contact karein: Phone: +92 336 1101234, Email: easternservices.pk@gmail.com."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Conversation history cleared!\n",
            "\n",
            "================================================================================\n",
            "TEST SET 2: CONTEXT MEMORY (PUNJABI)\n",
            "================================================================================\n",
            "  [üîÑ Punjabi detected ‚Üí Mapped to Roman Urdu]\n",
            "\n",
            "> QUESTION: Tussin keri keri service provide karday o?\n",
            "  [Language: ROMAN_URDU]\n",
            "  [Session ID: a5f050b4]\n",
            "  [Relevance Score: 0.52]\n",
            "  [Using Fallback: False]\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hum yeh services provide karte hain: Termite Treatment, Termite Proofing, General Pest Control, Bed Bug Treatment, Fumigation, Rodent Control, aur Cockroach Control. Agar aapko kisi bhi service ki zaroorat hai, to humse contact karein: Phone: +92 336 1101234, Email: easternservices.pk@gmail.com."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [üîÑ Punjabi detected ‚Üí Mapped to Roman Urdu]\n",
            "\n",
            "> QUESTION: Deemak da rate ki ah?\n",
            "  [Language: ROMAN_URDU]\n",
            "  [Session ID: a5f050b4]\n",
            "  [Relevance Score: 0.83]\n",
            "  [Using Fallback: False]\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Deemak ka rate Rs. 8 per sq ft hai. Residential ke liye minimum charge Rs. 25,000 hai. Agar aapka area zyada hai, to hum aapko detailed quote de sakte hain. Phone: +92 336 1101234, Email: easternservices.pk@gmail.com par contact karein."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [üîÑ Punjabi detected ‚Üí Mapped to Roman Urdu]\n",
            "\n",
            "> QUESTION: Guarantee kitni ae?\n",
            "  [Language: ROMAN_URDU]\n",
            "  [Session ID: a5f050b4]\n",
            "  [Relevance Score: 0.83]\n",
            "  [Using Fallback: False]\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Humari guarantee periods yeh hain: Termite Treatment = 5 saal, Termite Proofing = 10 saal, General Pest Control = 1 mahina, Bed Bug Treatment = 3 mahine. Agar aapka specific service puchna hai, to Termite Proofing ki guarantee 10 saal ki hai. Phone: +92 336 1101234, Email: easternservices.pk@gmail.com par humse contact karein."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Conversation history cleared!\n",
            "\n",
            "================================================================================\n",
            "TEST SET 3: ACCURACY - FIXED VALUES\n",
            "================================================================================\n",
            "\n",
            "> QUESTION: What is the termite treatment rate?\n",
            "  [Language: ENGLISH]\n",
            "  [Session ID: a5f050b4]\n",
            "  [Relevance Score: 1.00]\n",
            "  [Using Fallback: False]\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The termite treatment rate is Rs. 80 per sq ft for post-construction and Rs. 65 per sq ft for pre-construction termite proofing. For more information, you can contact us at Phone: +92 336 1101234 or Email: easternservices.pk@gmail.com."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "> QUESTION: How long is the guarantee period?\n",
            "  [Language: ENGLISH]\n",
            "  [Session ID: a5f050b4]\n",
            "  [Relevance Score: 0.69]\n",
            "  [Using Fallback: False]\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The guarantee period varies by service type, with Termite Treatment having a 5-year guarantee, Termite Proofing having a 10-year guarantee, General Pest Control having a 1-month guarantee, and Bed Bug Treatment having a 3-month guarantee. For more information, you can contact us at +92 336 1101234 or email easternservices.pk@gmail.com. The guarantee periods are specified for each service type."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "> QUESTION: What is the contact number?\n",
            "  [Language: ENGLISH]\n",
            "  [Session ID: a5f050b4]\n",
            "  [Relevance Score: 0.33]\n",
            "  [Using Fallback: False]\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Phone: +92 336 1101234, Email: easternservices.pk@gmail.com"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "> QUESTION: What is the email address?\n",
            "  [Language: ENGLISH]\n",
            "  [Session ID: a5f050b4]\n",
            "  [Relevance Score: 0.33]\n",
            "  [Using Fallback: False]\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The email address is easternservices.pk@gmail.com."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "> QUESTION: Which areas do you serve?\n",
            "  [Language: ENGLISH]\n",
            "  [Session ID: a5f050b4]\n",
            "  [Relevance Score: 0.33]\n",
            "  [Using Fallback: False]\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hello! We serve Islamabad, Rawalpindi, Lahore, Karachi, and other cities including Multan, Faisalabad, Peshawar, Sialkot, and Gujranwala. For more information, you can contact us at Phone: +92 336 1101234 or Email: easternservices.pk@gmail.com."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Conversation history cleared!\n",
            "\n",
            "================================================================================\n",
            "TEST SET 4: PURE URDU (NASTALIQ SCRIPT)\n",
            "================================================================================\n",
            "\n",
            "> QUESTION: ŸÖ€åÿ±€í ⁄Ø⁄æÿ± ŸÖ€å⁄∫ ÿØ€åŸÖ⁄© €Å€íÿå ÿ±€åŸπ ⁄©€åÿß €ÅŸà ⁄Øÿßÿü\n",
            "  [Language: URDU]\n",
            "  [Session ID: a5f050b4]\n",
            "   üîÑ Transliterated: 'ŸÖ€åÿ±€í ⁄Ø⁄æÿ± ŸÖ€å⁄∫ ÿØ€åŸÖ⁄© €Å€íÿå ÿ±€åŸπ ⁄©€åÿß €ÅŸà ⁄Øÿßÿü...' ‚Üí 'meray mery my ghar home house residence ...'\n",
            "  [Relevance Score: 0.15]\n",
            "  [Using Fallback: False]\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "ÿØ€åŸÖ⁄© ⁄©€í ÿπŸÑÿßÿ¨ ⁄©ÿß ÿ±€åŸπ ŸÅ€å ŸÖÿ±ÿ®ÿπ ŸÅŸπ Rs. 80 €Å€í€î 5 ÿ≥ÿßŸÑ ⁄©€å ⁄Øÿßÿ±ŸÜŸπ€å €Å€í€î ÿ®ÿ±ÿß€Å ⁄©ÿ±ŸÖ €ÅŸÖÿßÿ±€í ŸÜŸÖÿßÿ¶ŸÜÿØ€í ÿ≥€í ÿ±ÿßÿ®ÿ∑€Å ⁄©ÿ±€å⁄∫: ŸÅŸàŸÜ: +92 336 1101234ÿå ÿß€å ŸÖ€åŸÑ: easternservices.pk@gmail.com€î"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "> QUESTION: ⁄Øÿßÿ±ŸÜŸπ€å ⁄©ÿ™ŸÜ€å €Å€íÿü\n",
            "  [Language: URDU]\n",
            "  [Session ID: a5f050b4]\n",
            "   üîÑ Transliterated: '⁄Øÿßÿ±ŸÜŸπ€å ⁄©ÿ™ŸÜ€å €Å€íÿü...' ‚Üí 'guarantee warranty assurance kitni how m...'\n",
            "  [Relevance Score: 0.15]\n",
            "  [Using Fallback: False]\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "⁄Øÿßÿ±ŸÜŸπ€å ⁄©€å ŸÖÿØÿ™ ŸÖÿÆÿ™ŸÑŸÅ ÿ≥ÿ±Ÿàÿ≥ÿ≤ ⁄©€í ŸÑ€å€í ŸÖÿÆÿ™ŸÑŸÅ €Å€í€î ÿØ€åŸÖ⁄© ⁄©€í ÿπŸÑÿßÿ¨ ⁄©€å ⁄Øÿßÿ±ŸÜŸπ€å 5 ÿ≥ÿßŸÑ €Å€íÿå ÿØ€åŸÖ⁄© Ÿæÿ±ŸàŸÅŸÜ⁄Ø ⁄©€å ⁄Øÿßÿ±ŸÜŸπ€å 10 ÿ≥ÿßŸÑ €Å€íÿå ÿπÿßŸÖ ⁄©€å⁄ëŸà⁄∫ ⁄©€í ⁄©ŸÜŸπÿ±ŸàŸÑ ⁄©€å ⁄Øÿßÿ±ŸÜŸπ€å 1 ŸÖÿß€Å €Å€íÿå ÿßŸàÿ± ⁄©⁄æŸπŸÖŸÑ ⁄©€í ÿπŸÑÿßÿ¨ ⁄©€å ⁄Øÿßÿ±ŸÜŸπ€å 3 ŸÖÿß€Å €Å€í€î"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "> QUESTION: ⁄©ŸàŸÜ ⁄©ŸàŸÜ ÿ≥€å ÿÆÿØŸÖÿßÿ™ €Å€å⁄∫ÿü\n",
            "  [Language: URDU]\n",
            "  [Session ID: a5f050b4]\n",
            "   üîÑ Transliterated: '⁄©ŸàŸÜ ⁄©ŸàŸÜ ÿ≥€å ÿÆÿØŸÖÿßÿ™ €Å€å⁄∫ÿü...' ‚Üí 'kon kaun which who what kon kaun which w...'\n",
            "  [Relevance Score: 0.15]\n",
            "  [Using Fallback: False]\n",
            "\n",
            "--- RAG Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "€ÅŸÖ €å€Å ÿ≥ÿ±Ÿàÿ≥ÿ≤ ŸÅÿ±ÿß€ÅŸÖ ⁄©ÿ±ÿ™€í €Å€å⁄∫: ÿØ€åŸÖ⁄© ⁄©ÿß ÿπŸÑÿßÿ¨ÿå ÿØ€åŸÖ⁄© Ÿæÿ±ŸàŸÅŸÜ⁄Øÿå ÿπÿßŸÖ ⁄©€å⁄ëŸà⁄∫ ⁄©ÿß ⁄©ŸÜŸπÿ±ŸàŸÑÿå ⁄©⁄æŸπŸÖŸÑ ⁄©ÿß ÿπŸÑÿßÿ¨ÿå ŸÅ€åŸàŸÖ€å⁄Ø€åÿ¥ŸÜÿå ⁄ÜŸà€ÅŸà⁄∫ ⁄©ÿß ⁄©ŸÜŸπÿ±ŸàŸÑÿå ÿßŸàÿ± ⁄©ÿß⁄©ÿ±Ÿà⁄Ü ⁄©ŸÜŸπÿ±ŸàŸÑ€î €ÅŸÖÿßÿ±€í ÿ≥ÿ±Ÿàÿ≥ÿ≤ ⁄©€í ÿ®ÿßÿ±€í ŸÖ€å⁄∫ ŸÖÿ≤€åÿØ ŸÖÿπŸÑŸàŸÖÿßÿ™ ⁄©€í ŸÑ€å€íÿå ÿ®ÿ±ÿß€Å ⁄©ÿ±ŸÖ €ÅŸÖÿßÿ±€í ŸÜŸÖÿßÿ¶ŸÜÿØ€í ÿ≥€í ÿ±ÿßÿ®ÿ∑€Å ⁄©ÿ±€å⁄∫: ŸÅŸàŸÜ: +92 336 1101234ÿå ÿß€å ŸÖ€åŸÑ: easternservices.pk@gmail.com"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Conversation history cleared!\n",
            "\n",
            "================================================================================\n",
            "TEST SET 5: MULTILINGUAL MIX (REAL SCENARIO)\n",
            "================================================================================\n",
            "\n",
            "> QUESTION: What services do you offer?\n",
            "  [Language: ENGLISH]\n",
            "  [Session ID: a5f050b4]\n",
            "  [Relevance Score: 0.67]\n",
            "  [Using Fallback: False]\n",
            "‚ùå ERROR: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb71fnxvfhprgqz76g731v8g` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99409, Requested 1896. Please try again in 18m47.52s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "\n",
            "> QUESTION: Deemak ka rate kya hai?\n",
            "  [Language: ROMAN_URDU]\n",
            "  [Session ID: a5f050b4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-695585012.py\", line 83, in ask_chatbot\n",
            "    response = chain.invoke({\"context\": context, \"question\": question})\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3129, in invoke\n",
            "    input_ = context.run(step.invoke, input_, config)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 398, in invoke\n",
            "    self.generate_prompt(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1117, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 927, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1221, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_groq/chat_models.py\", line 590, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/groq/resources/chat/completions.py\", line 464, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\", line 1242, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\", line 1044, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb71fnxvfhprgqz76g731v8g` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99409, Requested 1896. Please try again in 18m47.52s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Relevance Score: 0.72]\n",
            "  [Using Fallback: False]\n",
            "‚ùå ERROR: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb71fnxvfhprgqz76g731v8g` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99408, Requested 2182. Please try again in 22m53.76s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "\n",
            "> QUESTION: Mera 10 marlay ka ghar hai\n",
            "  [Language: ROMAN_URDU]\n",
            "  [Session ID: a5f050b4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-695585012.py\", line 83, in ask_chatbot\n",
            "    response = chain.invoke({\"context\": context, \"question\": question})\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3129, in invoke\n",
            "    input_ = context.run(step.invoke, input_, config)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 398, in invoke\n",
            "    self.generate_prompt(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1117, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 927, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1221, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_groq/chat_models.py\", line 590, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/groq/resources/chat/completions.py\", line 464, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\", line 1242, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\", line 1044, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb71fnxvfhprgqz76g731v8g` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99408, Requested 2182. Please try again in 22m53.76s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Relevance Score: 0.50]\n",
            "  [Using Fallback: False]\n",
            "‚ùå ERROR: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb71fnxvfhprgqz76g731v8g` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99406, Requested 2468. Please try again in 26m59.136s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "  [üîÑ Punjabi detected ‚Üí Mapped to Roman Urdu]\n",
            "\n",
            "> QUESTION: Guarantee kitni ae?\n",
            "  [Language: ROMAN_URDU]\n",
            "  [Session ID: a5f050b4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-695585012.py\", line 83, in ask_chatbot\n",
            "    response = chain.invoke({\"context\": context, \"question\": question})\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3129, in invoke\n",
            "    input_ = context.run(step.invoke, input_, config)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 398, in invoke\n",
            "    self.generate_prompt(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1117, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 927, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1221, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_groq/chat_models.py\", line 590, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/groq/resources/chat/completions.py\", line 464, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\", line 1242, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\", line 1044, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb71fnxvfhprgqz76g731v8g` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99406, Requested 2468. Please try again in 26m59.136s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Relevance Score: 0.83]\n",
            "  [Using Fallback: False]\n",
            "‚ùå ERROR: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb71fnxvfhprgqz76g731v8g` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99405, Requested 1882. Please try again in 18m31.967999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "\n",
            "> QUESTION: ⁄©ŸàŸÜ ÿ≥€í ÿπŸÑÿßŸÇ€íÿü\n",
            "  [Language: URDU]\n",
            "  [Session ID: a5f050b4]\n",
            "   üîÑ Transliterated: '⁄©ŸàŸÜ ÿ≥€í ÿπŸÑÿßŸÇ€íÿü...' ‚Üí 'kon kaun which who what se say from with...'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-695585012.py\", line 83, in ask_chatbot\n",
            "    response = chain.invoke({\"context\": context, \"question\": question})\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3129, in invoke\n",
            "    input_ = context.run(step.invoke, input_, config)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 398, in invoke\n",
            "    self.generate_prompt(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1117, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 927, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1221, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_groq/chat_models.py\", line 590, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/groq/resources/chat/completions.py\", line 464, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\", line 1242, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\", line 1044, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb71fnxvfhprgqz76g731v8g` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99405, Requested 1882. Please try again in 18m31.967999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Relevance Score: 0.15]\n",
            "  [Using Fallback: False]\n",
            "‚ùå ERROR: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb71fnxvfhprgqz76g731v8g` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99404, Requested 2927. Please try again in 33m33.984s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "\n",
            "============================================================\n",
            "EVALUATION SUMMARY\n",
            "============================================================\n",
            "Total Queries: 35\n",
            "\n",
            "Average Scores:\n",
            "  Relevance: 0.37/1.00\n",
            "  Completeness: 1.00/1.00\n",
            "  Overall: 0.62/1.00\n",
            "\n",
            "Average Response Time: 9.38s\n",
            "\n",
            "Queries by Language:\n",
            "  roman_urdu: 17\n",
            "  english: 8\n",
            "  urdu: 10\n",
            "============================================================\n",
            "\n",
            "‚úÖ Comprehensive testing complete!\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-695585012.py\", line 83, in ask_chatbot\n",
            "    response = chain.invoke({\"context\": context, \"question\": question})\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3129, in invoke\n",
            "    input_ = context.run(step.invoke, input_, config)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 398, in invoke\n",
            "    self.generate_prompt(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1117, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 927, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1221, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_groq/chat_models.py\", line 590, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/groq/resources/chat/completions.py\", line 464, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\", line 1242, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\", line 1044, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb71fnxvfhprgqz76g731v8g` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99404, Requested 2927. Please try again in 33m33.984s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LOrXa2J8xaQG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}